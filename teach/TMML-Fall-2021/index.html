<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Ju Sun | Topics in Modern Machine Learning</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/teach/TMML-Fall-2021/">
  
  <style>
	ul {list-style-type: circle;}
  </style>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Ju Sun</strong>
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">Welcome</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">Blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/teach/">Teaching</a>
          
        
          
            <a class="page-link" href="/pub/">Publications</a>
          
        
          
            <a class="page-link" href="/talks/">Talks</a>
          
        
          
            <a class="page-link" href="/software/">Software</a>
          
        
          
            <a class="page-link" href="/grants/">Grants</a>
          
        
          
            <a class="page-link" href="/people/">People</a>
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/research/">Research</a>
          
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Topics in Modern Machine Learning</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Topics in Modern Machine Learning clearfix">
    <h3 id="theme-of-this-iteration">Theme of this iteration</h3>

<p><strong>theoretical foundations of deep learning</strong>… examines deep learning through the lens of classic and modern approximation, optimization, and learning theory</p>

<h3 id="course-info">Course info</h3>

<p><strong>Syllabus</strong>: <a href="/teach/TMML-Fall-2021/TMML.pdf">PDF</a></p>

<p><strong>Instructor</strong>:  <a href="https://sunju.org/">Professor Ju Sun</a>  Email: jusun AT umn.edu</p>

<p><strong>When/Where</strong>: Mon/Wed 2:30 – 3:45pm @ Amundson Hall 156</p>

<p><strong>Office Hours</strong>: Mon/Wed 4 – 5pm @ Zoom or my office</p>

<h3 id="schedule">Schedule</h3>

<p>General readings: <a href="https://www.pnas.org/cc/arthur-m-sackler-colloquium-on-the-science-of-deep-learning">PNAS Colloquium on The Science of Deep Learning, Dec 2020</a>. The course is organized around six thrusts.</p>

<h4 id="approximation">Approximation</h4>
<ul>
  <li>Surveys
    <ul>
      <li><a href="https://arxiv.org/abs/2007.04759">Expressivity of Deep Neural Networks</a></li>
      <li><a href="https://arxiv.org/abs/2012.14501">Neural Network Approximation</a></li>
      <li>Chaps 3–4 of <a href="https://arxiv.org/abs/2105.04026">The Modern Mathematics of Deep Learning</a></li>
    </ul>
  </li>
  <li>Papers
    <ul>
      <li><a href="http://neuralnetworksanddeeplearning.com/chap4.html">Visual proof of universality for shallow networks</a></li>
      <li><a href="https://doi.org/10.1017/S0962492900002919">Approximation theory of the MLP model in neural networks</a> (Survey of UAT focused on shallow networks)</li>
      <li><a href="https://doi.org/10.1016/0893-6080(89)90020-8">Multilayer feedforward networks are universal
approximators</a> (UAT of both deep and shallow networks)</li>
      <li><a href="https://arxiv.org/abs/1708.02691">Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations</a> (ReLU networks with bounded width and unbounded depth)</li>
      <li><a href="https://arxiv.org/abs/1710.11278">Approximating continuous functions by ReLU nets of minimal width</a> (ReLU networks with bounded width and unbounded depth)</li>
      <li><a href="https://arxiv.org/abs/1610.01145">Error bounds for approximations with deep ReLU networks</a> (ReLU network approximation of smooth functions)</li>
      <li><a href="https://arxiv.org/abs/1902.07896">Error bounds for approximations with deep relu neural networks in $W^{s, p}$ norms</a> (ReLU network approximation of smooth functions)</li>
      <li><a href="https://arxiv.org/abs/1509.07385">Provable approximation properties for deep neural networks</a> (Approximation of functions on manifolds)</li>
      <li><a href="https://arxiv.org/abs/1705.05502">The power of deeper networks for expressing natural functions</a> (Exponential separation of deep and shallow networks in approximating polynomials)</li>
      <li><a href="https://arxiv.org/abs/1602.04485">Benefits of depth in neural networks</a> (Exponential separation of deep and shallow networks due to composition of ReLU activated layers)</li>
      <li><a href="https://doi.org/10.1007/s11633-017-1054-2">Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review</a> (separation of deep and shallow networks on compositional functions)</li>
      <li><a href="/teach/TMML-Fall-2021/Donoho-2000.pdf">High-dimensional data analysis: The blessings and curses of dimensionality</a> (Donoho’s 2000 talk on curse/blessing of dimensionality in modern data analysis)</li>
      <li><a href="https://arxiv.org/abs/1804.10306">Universal approximations of invariant maps by neural networks</a> (Approximation of invariant and equivariant functions wrt finite groups)</li>
      <li><a href="https://arxiv.org/abs/1809.00973">Equivalence of approximation by convolutional neural networks and fully-connected networks</a> (Approximation of equivariant functions using CNNs)</li>
    </ul>
  </li>
  <li>Further readings
    <ul>
      <li><a href="https://arxiv.org/abs/2001.07523">The gap between theory and practice in function approximation with deep neural networks</a></li>
      <li><a href="https://arxiv.org/abs/2104.02746">Proof of the Theory-to-Practice Gap in Deep Learning via Sampling Complexity bounds for Neural Network Approximation Spaces</a></li>
      <li><a href="https://arxiv.org/abs/2008.02545">A deep network construction that adapts to intrinsic dimensionality beyond the domain</a> (Approximation of functions on manifolds with refined rates)</li>
    </ul>
  </li>
</ul>

<h4 id="optimization--generalization">Optimization \&amp; Generalization</h4>
<ul>
  <li>Surveys
    <ul>
      <li>Chaps 1–2, 5, 6 of <a href="https://arxiv.org/abs/2105.04026">The Modern Mathematics of Deep Learning</a></li>
      <li><a href="https://arxiv.org/abs/2103.09177">Deep learning: a statistical viewpoint</a></li>
      <li><a href="https://arxiv.org/abs/2105.14368">Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation</a></li>
      <li><a href="https://arxiv.org/abs/2109.02355">A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning</a></li>
    </ul>
  </li>
  <li>Papers
    <ul>
      <li><a href="https://arxiv.org/abs/2012.06188">Recent Theoretical Advances in Non-Convex Optimization</a> (computational complexity of algorithms for nonconvex problems)</li>
      <li><a href="https://arxiv.org/abs/1912.08957">Optimization for deep learning: theory and algorithms</a> (deep learning algorithms, tricks, and theories thereof)</li>
      <li><a href="https://arxiv.org/abs/2007.06753">From Symmetry to Geometry: Tractable Nonconvex Problems</a> (problems that are “essentially” convex up to symmetries)</li>
      <li><a href="https://arxiv.org/abs/1710.00797">Accelerated methods for $\alpha$-weakly-quasi-convex problems</a> (early results on optimizing star-convex functions)</li>
      <li><a href="https://arxiv.org/abs/1809.05895">Primal-dual accelerated gradient methods with small-dimensional relaxation oracle</a> (early results on optimizing star-convex functions)</li>
      <li><a href="https://arxiv.org/abs/1906.11985">Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond</a> (state-of-the-art results on optimizing star-convex functions)</li>
      <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a> (optimizing of neural networks via star convexity)</li>
      <li><a href="https://arxiv.org/abs/1901.00451">SGD Converges to Global Minimum in Deep Learning via Star-convex Path</a> (optimizing of neural networks via star convexity)</li>
      <li><a href="https://arxiv.org/abs/1608.04636">Linear Convergence of Gradient and Proximal-Gradient Methods Under the Polyak-Łojasiewicz Condition</a> (state-of-the-art convergence results based on PL conditions)</li>
      <li><a href="https://arxiv.org/abs/2010.01092">On the linearity of large non-linear models: when and why the tangent kernel is constant</a> (explains when and why model linearization makes sense for nonlinear models)</li>
      <li><a href="https://arxiv.org/abs/2003.00307">Loss landscapes and optimization in over-parameterized non-linear systems and neural networks</a> (non-linear systems and neural networks satisfy the PL conditions under certain conditions)</li>
      <li><a href="https://arxiv.org/abs/1812.07956">On Lazy Training in Differentiable Programming</a> (when and why linearization makes sense, and when not—esp. in practice)</li>
      <li><a href="https://arxiv.org/abs/1812.10004">Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?</a> (another proof of PL conditions imply global convergence and short optimization paths for undertermined nonlinear models)</li>
      <li><a href="https://arxiv.org/abs/1902.04674">Towards moderate overparameterization: global convergence guarantees for training shallow neural networks</a> (specialization and improvement of the above result for shallow neural networks)</li>
      <li><a href="https://doi.org/10.1007/s13373-017-0101-1">{Euclidean, metric, and Wasserstein} gradient flows: an overview</a> (a nice tutorial on gradient flows)</li>
      <li><a href="https://arxiv.org/abs/1803.00567">Computational Optimal Transport</a> (optimal transport problems, Wasserstein distances, and (Chap 9) Wasserstein gradient flow)</li>
      <li><a href="https://arxiv.org/abs/2110.08084">Gradient Descent on Infinitely Wide Neural Networks: Global Convergence and Generalization</a> (overview of global convergence and generalization of 2-layer NNs using mean-field analysis)</li>
      <li><a href="https://arxiv.org/abs/1805.09545">On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport</a> (the 1st technical paper behind the above review paper)</li>
      <li><a href="https://arxiv.org/abs/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a> (the 2nd technical paper behind the above review paper)</li>
      <li><a href="https://arxiv.org/abs/1804.06561">A Mean Field View of the Landscape of Two-Layers Neural Networks</a> (complementary mean-field analysis)</li>
      <li><a href="https://arxiv.org/abs/1902.06015">Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit</a> (complementary mean-field analysis)</li>
      <li><a href="https://arxiv.org/abs/2001.11443">A Rigorous Framework for the Mean Field Limit of Multilayer Neural Networks</a> (mean-field analysis of deep networks)</li>
      <li><a href="https://doi.org/10.1007/978-3-540-28650-9_8">Introduction to Statistical Learning Theory</a> (quick review of classic statistical learning theory)</li>
      <li><a href="https://doi.org/10.1051/ps:2005018">Theory of Classification: a Survey of Some Recent Advances</a> (quick review of more recent developments of statistical learning theory)</li>
      <li><a href="https://doi.org/10.1145/2699439">Learning without Concentration</a> (dealing with heavy-tailed data and functions in learning theory using one-side concentration)</li>
      <li><a href="https://arxiv.org/abs/1709.00843">Extending the scope of the small-ball method</a> (dealing with heavy-tailed data and functions in learning theory using one-side concentration)</li>
      <li><a href="https://arxiv.org/abs/1703.02930">Nearly-tight VC-dimension and pseudodimension bounds for piecewise linear neural networks</a> (state-of-the-art VC dimension results for DNNs)</li>
      <li><a href="https://www.jmlr.org/papers/v3/bartlett02a.html">Rademacher and Gaussian Complexities: Risk Bounds and Structural Results</a> (introduction of Rademacher and Gaussian complexities; Rad complexity of 2-layer NN)</li>
      <li><a href="https://arxiv.org/abs/1706.08498">Spectrally-normalized margin bounds for neural networks</a> (state-of-the-art Rad complexity estimation for DNNs)</li>
      <li><a href="https://arxiv.org/abs/1712.06541">Size-Independent Sample Complexity of Neural Networks</a> (another state-of-the-art Rad complexity estimation for DNNs)</li>
      <li><a href="https://doi.org/10.1145/3446776">Understanding deep learning (still) requires rethinking generalization</a> (distribution-independent bounds unlikely to explain the generalization of DL)</li>
      <li><a href="https://arxiv.org/abs/1802.01396">To understand deep learning we need to understand kernel learning</a> (distribution-dependent bounds may also struggle to explain the generalization of DL)</li>
      <li><a href="https://arxiv.org/abs/1902.04742">Uniform convergence may be unable to explain generalization in deep learning</a> (potential failure of two-sided uniform convergence results for DL)</li>
      <li><a href="https://arxiv.org/abs/2010.08479">Failures of model-dependent generalization bounds for least-norm interpolation</a> (a complementary (stronger) version of the above)</li>
      <li><a href="https://icml.cc/Conferences/2009/papers/89.pdf">PAC-Bayesian Learning of Linear Classifiers</a> (PAC-Bayesian derandomization for linear predictors)</li>
      <li><a href="https://arxiv.org/abs/1707.09564">A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks</a> (PAC-Bayesian results for DNNs)</li>
      <li><a href="https://www.ias.edu/sites/default/files/math/special_year_workshops/kdziugaite.pdf">Studying generalization in deep learning via PAC-Bayes</a> (tutorial talk by Gintare Karolina Dziugaite)</li>
      <li><a href="https://arxiv.org/abs/1905.13435">PAC-Bayesian Transportation Bound</a> (Abstract view of PAC Bayesian, and its connection to transportation bounds)</li>
      <li><a href="https://www.jmlr.org/papers/v2/bousquet02a.html">Stability and Generalization</a> (classic results connecting stability and generalization)</li>
      <li><a href="https://jmlr.csail.mit.edu/papers/volume11/shalev-shwartz10a/shalev-shwartz10a.pdf">Learnability, Stability and Uniform Convergence</a> (classic results connecting stability and generalization)</li>
      <li><a href="https://arxiv.org/abs/1910.07833">Sharper bounds for uniformly stable algorithms</a> (state-of-the-art generalization bounds under uniform stability)</li>
      <li><a href="https://arxiv.org/abs/2006.06914">Stability of Stochastic Gradient Descent on Nonsmooth Convex Losses</a> (algorithm stability of SGD on convex problems)</li>
      <li><a href="https://arxiv.org/abs/2006.08157">Fine-Grained Analysis of Stability and Generalization for Stochastic Gradient Descent</a> (refined notion of stability for SGD on nonconvex problems)</li>
      <li><a href="https://arxiv.org/abs/1902.10710">High probability generalization bounds for uniformly stable algorithms with nearly optimal rate</a> (high probability generalization bounds based on stability for convex problems)</li>
      <li><a href="https://arxiv.org/abs/2006.05610">High-probability Convergence Bounds for Non-convex Stochastic Gradient Descent</a> (high probability generalization bounds based on stability for nonconvex problems under certain conditions)</li>
      <li><a href="https://jmlr.org/papers/v19/18-188.html">The Implicit Bias of Gradient Descent on Separable Data</a> (Algorithm bias of GD on several classical learning models)</li>
      <li><a href="https://arxiv.org/abs/math/0508276">Boosting with early stopping: Convergence and consistency</a> (Algorithm bias with coordinate descent on boosting models)</li>
      <li><a href="https://arxiv.org/abs/1303.4172">Margins, Shrinkage, and Boosting</a> (Algorithm bias with coordinate descent on boosting models)</li>
      <li><a href="https://arxiv.org/abs/1903.09367">Implicit Regularization via Hadamard Product Over-Parametrization in High-Dimensional Linear Regression</a> (Algorithm bias toward sparse solutions in linear regression)</li>
      <li><a href="https://arxiv.org/abs/1909.05122">Implicit Regularization for Optimal Sparse Recovery</a> (Algorithm bias toward sparse solutions in linear regression)</li>
      <li><a href="https://arxiv.org/abs/1712.09203">Algorithmic Regularization in Over-parameterized Matrix Sensing and Neural Networks with Quadratic Activations</a> (Algorithm bias in factorization)</li>
      <li><a href="https://arxiv.org/abs/1905.13655">Implicit Regularization in Deep Matrix Factorization</a> (Algorithm bias in linear DNNs)</li>
      <li><a href="https://arxiv.org/abs/1806.00900">Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced</a> (Algorithm bias in DNNs)</li>
      <li><a href="https://arxiv.org/abs/2005.06398">Implicit Regularization in Deep Learning May Not Be Explainable by Norms</a> (Difficulty in characterizing algorithm bias)</li>
      <li><a href="https://arxiv.org/abs/2110.02732">On Margin Maximization in Linear and ReLU Networks</a> (Difficulty in characterizing algorithm bias)</li>
      <li><a href="https://arxiv.org/abs/1812.11118">Reconciling modern machine learning practice and the bias-variance trade-off</a> (Initializes double descent study in modern machine learning)</li>
      <li><a href="https://doi.org/10.1073/pnas.2001875117">A brief prehistory of double descent</a> (Double descent in early literature)</li>
      <li><a href="https://arxiv.org/abs/1912.02292">Deep Double Descent: Where Bigger Models and More Data Hurt</a> (Double descent on computer vision datasets)</li>
      <li><a href="https://www.jmlr.org/papers/volume18/15-240/15-240.pdf">Explaining the Success of AdaBoost and Random Forests as Interpolating Classifiers</a> (New explanation of the success of random forests and Adaboost as interpolating predictors)</li>
      <li><a href="https://arxiv.org/abs/1906.11300">Benign Overfitting in Linear Regression</a> (Bias-variance characterization for linear regression with least L2 norm solution)</li>
      <li><a href="https://arxiv.org/abs/2009.14286">Benign overfitting in ridge regression</a> (Bias-variance characterization for linear regression with least L2 norm solution, as well as ridge regression)</li>
      <li><a href="https://arxiv.org/abs/1808.00387">Just interpolate: Kernel “ridgeless” regression can generalize</a> (Bias-variance characterization of the least L2 norm solution for kernel regression)</li>
      <li><a href="https://arxiv.org/abs/1908.10292">On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry of Kernels</a> (Bias-variance characterization of the least L2 norm solution for kernel regression)</li>
      <li><a href="https://arxiv.org/abs/1903.08560">Surprises in High-Dimensional Ridgeless Least Squares Interpolation</a> (Bias-variance characterization of the least L2 norm solution for kernel regression)</li>
      <li><a href="https://arxiv.org/abs/1904.12191">Linearized two-layers neural networks in high dimension</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
      <li><a href="https://arxiv.org/abs/2006.13409">When Do Neural Networks Outperform Kernel Methods?</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
      <li><a href="https://arxiv.org/abs/2101.10588">Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
      <li><a href="https://arxiv.org/abs/1908.05355">The generalization error of random features regression: Precise asymptotics and double descent curve</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
      <li><a href="https://arxiv.org/abs/2007.12826">The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
      <li><a href="https://arxiv.org/abs/2108.11489">The Interplay Between Implicit Bias and Benign Overfitting in Two-Layer Linear Networks</a> (Bias-variance characterization of 1.5-layer NN in the linearization regime)</li>
    </ul>
  </li>
</ul>

<h4 id="invariance">Invariance</h4>
<ul>
  <li>Surveys
    <ul>
      <li><a href="https://arxiv.org/abs/2104.13478">Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges</a></li>
      <li><a href="https://geometricdeeplearning.com/lectures/">Geometric Deep Learning Lecture Materials</a></li>
      <li><a href="http://people.cs.uchicago.edu/~risi/research/cnn.html">Generalized covariant neural networks (webpage by Prof. Risi Kondor)</a></li>
      <li><a href="http://people.cs.uchicago.edu/~risi/research/symmetric.html">Fourier analysis on permutations (webpage by Prof. Risi Kondor)</a></li>
    </ul>
  </li>
</ul>

<h4 id="robustness">Robustness</h4>

<h4 id="generation">Generation</h4>

  </article>

<!--  -->

<!--  -->

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2026 Ju Sun.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!--load Ubuntu webfont -->

<link href='http://fonts.googleapis.com/css?family=Ubuntu|Ubuntu+Condensed|Ubuntu+Mono' rel='stylesheet' type='text/css'>

<!-- mathjax config-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true, 
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
    TeX: { equationNumbers: { autoNumber: "AMS", useLabelIds: true } }, 
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
