---
layout: page
permalink: /teach/TMML-Fall-2021/
title: Topics in Modern Machine Learning
---

### Theme of this iteration

 **theoretical foundations of deep learning**... examines deep learning through the lens of classic and modern approximation, optimization, and learning theory

### Course info

**Syllabus**: [PDF](TMML.pdf)

**Instructor**:  [Professor Ju Sun](https://sunju.org/)  Email: jusun AT umn.edu  

**When/Where**: Mon/Wed 2:30 -- 3:45pm @ Amundson Hall 156

**Office Hours**: Mon/Wed 4 -- 5pm @ Zoom or my office 

### Schedule

General readings: [PNAS Colloquium on The Science of Deep Learning, Dec 2020](https://www.pnas.org/cc/arthur-m-sackler-colloquium-on-the-science-of-deep-learning). The course is organized around six thrusts. 

#### Approximation   
+ Surveys 
  + [Expressivity of Deep Neural Networks](https://arxiv.org/abs/2007.04759)
  + [Neural Network Approximation](https://arxiv.org/abs/2012.14501)
  + Chaps 3--4 of [The Modern Mathematics of Deep Learning](https://arxiv.org/abs/2105.04026)
+ Papers 
  + [Visual proof of universality for shallow networks](http://neuralnetworksanddeeplearning.com/chap4.html)
  + [Approximation theory of the MLP model in neural networks](https://doi.org/10.1017/S0962492900002919) (Survey of UAT focused on shallow networks)
  + [Multilayer feedforward networks are universal
approximators](https://doi.org/10.1016/0893-6080(89)90020-8) (UAT of both deep and shallow networks) 
  + [Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations](https://arxiv.org/abs/1708.02691) (ReLU networks with bounded width and unbounded depth)
  + [Approximating continuous functions by ReLU nets of minimal width](https://arxiv.org/abs/1710.11278) (ReLU networks with bounded width and unbounded depth)
  + [Error bounds for approximations with deep ReLU networks](https://arxiv.org/abs/1610.01145) (ReLU network approximation of smooth functions)
  + [Error bounds for approximations with deep relu neural networks in $W^{s, p}$ norms](https://arxiv.org/abs/1902.07896) (ReLU network approximation of smooth functions)
  + [Provable approximation properties for deep neural networks](https://arxiv.org/abs/1509.07385) (Approximation of functions on manifolds)
  + [The power of deeper networks for expressing natural functions](https://arxiv.org/abs/1705.05502) (Exponential separation of deep and shallow networks in approximating polynomials)
  + [Benefits of depth in neural networks](https://arxiv.org/abs/1602.04485) (Exponential separation of deep and shallow networks due to composition of ReLU activated layers)
  + [Why and when can deep-but not shallow-networks avoid the curse of dimensionality: A review]( https://doi.org/10.1007/s11633-017-1054-2) (separation of deep and shallow networks on compositional functions)
  + [High-dimensional data analysis: The blessings and curses of dimensionality](Donoho-2000.pdf) (Donoho's 2000 talk on curse/blessing of dimensionality in modern data analysis)
  + [Universal approximations of invariant maps by neural networks](https://arxiv.org/abs/1804.10306) (Approximation of invariant and equivariant functions wrt finite groups)
  + [Equivalence of approximation by convolutional neural networks and fully-connected networks](https://arxiv.org/abs/1809.00973) (Approximation of equivariant functions using CNNs)
+ Further readings 
  + [The gap between theory and practice in function approximation with deep neural networks](https://arxiv.org/abs/2001.07523)
  + [Proof of the Theory-to-Practice Gap in Deep Learning via Sampling Complexity bounds for Neural Network Approximation Spaces](https://arxiv.org/abs/2104.02746)
  + [A deep network construction that adapts to intrinsic dimensionality beyond the domain](https://arxiv.org/abs/2008.02545) (Approximation of functions on manifolds with refined rates)

#### Optimization \& Generalization 
+ Surveys  
  + Chaps 1--2, 5, 6 of [The Modern Mathematics of Deep Learning](https://arxiv.org/abs/2105.04026)
  + [Deep learning: a statistical viewpoint](https://arxiv.org/abs/2103.09177)
  + [Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation](https://arxiv.org/abs/2105.14368)
  + [A Farewell to the Bias-Variance Tradeoff? An Overview of the Theory of Overparameterized Machine Learning](https://arxiv.org/abs/2109.02355)
  
+ Papers
  + [Recent Theoretical Advances in Non-Convex Optimization](https://arxiv.org/abs/2012.06188) (computational complexity of algorithms for nonconvex problems)
  + [Optimization for deep learning: theory and algorithms](https://arxiv.org/abs/1912.08957) (deep learning algorithms, tricks, and theories thereof)
  + [From Symmetry to Geometry: Tractable Nonconvex Problems](https://arxiv.org/abs/2007.06753) (problems that are "essentially" convex up to symmetries)

#### Invariance 
+ Surveys 
  + [Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges](https://arxiv.org/abs/2104.13478)

#### Robustness 

#### Generation 