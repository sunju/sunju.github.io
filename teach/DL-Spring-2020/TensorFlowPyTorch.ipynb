{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlowPyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTBsQ7Vzu63_",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to MSI, TensorFlow, and PyTorch\n",
        "\n",
        "## Outline\n",
        "Below is a short introdcution some tools used in deep learning applications. We will go over\n",
        "* Connecting and running jobs at MSI\n",
        "* Using custom kernels in Jupyter\n",
        "* Construction of a basic neural network with TensorFlow \n",
        "* Saving/Restoring models\n",
        "* Basic example of PyTorch\n",
        "\n",
        "z.umn.edu/colab-5980\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECqfy8U2aA3g",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to MSI\n",
        "\n",
        "### Connecting to MSI\n",
        "- SSH from UMN network (including wireless and VPN)\n",
        "```\n",
        "ssh login.msi.umn.edu\n",
        "ssh mesabi\n",
        "```\n",
        "- Jupyter (https://notebooks.msi.umn.edu/)\n",
        "\n",
        "### Storage\n",
        "- Home directory ~/\n",
        "- Global scratch /scratch.global\n",
        "- Local scratch /scratch.local\n",
        "- S3 storage (s3.msi.umn.edu)\n",
        "```\n",
        "ssh login.msi.umn.edu\n",
        "s3cmd ls \n",
        "s3cmd mb s3://$USER\n",
        "s3cmd put some.file s3://$USER/some.file\n",
        "```\n",
        "\n",
        "### Software\n",
        "- environment modules\n",
        " - used to control environment variables and prevent conflicts between the hundreds of installed software packages\n",
        "```\n",
        "module avail python\n",
        "module load python/3.6.3\n",
        "module unload python\n",
        "```\n",
        "\n",
        "### Compute Hardware\n",
        "![MSI Servers](https://blynch.s3.msi.umn.edu/public/csci_5980_1_5.png)\n",
        "\n",
        "\n",
        "- Mesabi (2015)\n",
        " - Cores: 19,040 Intel Haswell\n",
        " - Memory: 83 TB\n",
        " - Accelerators: 80 K40 Nvidia gpGPUs\n",
        " - Peak: 860 TFlops\n",
        "\n",
        "- Mangi (2019 Mesabi upgrade) \n",
        " - Cores: 20,888 AMD Rome\n",
        " - Memory: 56 TB\n",
        " - Accelerators: 40 Nvidia V100s\n",
        " - Peak: 1150 TFlops\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4jHzxnEY7N-",
        "colab_type": "text"
      },
      "source": [
        "## Singularity\n",
        "- Run most containers on MSI resources without sudo permission\n",
        "```\n",
        "module load singularity\n",
        "```\n",
        "\n",
        "Singularity can be used to run most Docker containers. You can create a container on an Ubuntu laptop, transfer it to MSI, and then execute it using Singularity on the CentOS7 compute nodes.\n",
        "\n",
        "**Note: you CAN NOT currently build singularity images on MSI login nodes**<br/>\n",
        "You can\n",
        "- create images elsewhere\n",
        "- download from trusted sources\n",
        "- remote build your continaers using the singularity public servers (if you trust them)\n",
        "\n",
        "```\n",
        "blynch@ln0006 [~/] cat test.spec\n",
        "Bootstrap: docker\n",
        "From: ubuntu:xenial-20191108\n",
        "\n",
        "%post\n",
        "apt-get update\n",
        "apt-get upgrade -y\n",
        "\n",
        "blynch@ln0006 [~/] singularity build --remote mytest.img test.spec\n",
        "```\n",
        "\n",
        "### Links\n",
        "- [MSI Website](https://msi.umn.edu)\n",
        "- [MSI YouTube Channel](https://www.youtube.com/channel/UCbdSk8D0_NP9BNbe2uXhV8Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ueJMEGexrgZ",
        "colab_type": "text"
      },
      "source": [
        "# Batch Computing\n",
        "\n",
        "1. Create a batch script\n",
        "2. Submit script to a queue\n",
        "3. Scheduler runs script at some point on the reosurces requested\n",
        "\n",
        "A PBS submission script has 2 components\n",
        "1. #PBS directives to tell the scheduler what resources you want\n",
        "2. a set of commands to run\n",
        "\n",
        "**An example script would look like:**\n",
        "\n",
        "```\n",
        "#!/usr/bin/bash\n",
        "#PBS -l nodes=1:ppn=24\n",
        "#PBS -l walltime=5:00:00\n",
        "#PBS -l mem=60gb\n",
        "#PBS -e myjob.e\n",
        "#PBS -o myjob.o\n",
        "#PBS -q mesabi\n",
        "\n",
        "module load python\n",
        "source activate myenvironment\n",
        "cd some/directory\n",
        "python something.py\n",
        "```\n",
        "**and can be submitted like:**\n",
        "\n",
        "```\n",
        "qsub myscript.sh\n",
        "```\n",
        "\n",
        "\n",
        "### Job Queues\n",
        "https://www.msi.umn.edu/queues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4jprTf3pHO9",
        "colab_type": "text"
      },
      "source": [
        "# Interactive computing\n",
        "- Jupyter (https://notebooks.msi.umn.edu)\n",
        "- qsub <br>\n",
        "From the command line:\n",
        "```\n",
        "ssh login.msi.umn.edu\n",
        "ssh mesabi\n",
        "qsub -l nodes=1:ppn=2,mem=2gb,walltime=1:00:00 -q interactive -I\n",
        "```\n",
        "\n",
        "The **interactive** queue is more available than other queues, but you can always request resources for interactive use. E.g.;\n",
        "```\n",
        "qsub -l nodes=1:ppn=24,gpus=2,walltime=1:00:00 -q k40 -I\n",
        "```\n",
        "or\n",
        "```\n",
        "qsub -l nodes=1:ppn=24,gpus=2,walltime=1:00:00 -q v100 -I\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-1Br_5w6xTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdzaZ5Gv1OtN",
        "colab_type": "text"
      },
      "source": [
        "# Using Custom Kernels in Jupyter\n",
        "\n",
        "\n",
        "1.   ssh into mesabi\n",
        "2.   load python module\n",
        "3.   create a new python environment\n",
        "4.   customize environment\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxa6zYA-1Scs",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "ssh login.msi.umn.edu\n",
        "ssh mesabi\n",
        "\n",
        "module load python\n",
        "conda create -y --name myproject\n",
        "source activate myproject\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvVEWr758JGV",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "mkdir .local/share/jupyter/kernels/mynewkernel\n",
        "vi .local/share/jupyter/kernels/mynewkernel/kernel.json\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWLG8XWI0YIB",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "{\n",
        " \"argv\": [\n",
        "  \"/home/support/blynch/.conda/envs/mynewkernel/bin/python\",\n",
        "  \"-m\",\n",
        "  \"ipykernel_launcher\",\n",
        "  \"-f\",\n",
        "  \"{connection_file}\"\n",
        " ],\n",
        " \"display_name\": \"Python 3 - My Special Kernel\",\n",
        " \"language\": \"python\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLFUmSqmz_b5",
        "colab_type": "text"
      },
      "source": [
        "customize your kernels\n",
        "```\n",
        "{\n",
        " \"argv\": [\n",
        "  \"/opt/singularity/singularity\",\n",
        "  \"exec\",\n",
        "  \"-B\",\n",
        "  \"/panfs/roc/groups/2/support/blynch:/panfs/roc/groups/2/support/blynch\",\n",
        "  \"/home/support/blynch/singularity/tf.simg\",\n",
        "  \"/opt/anaconda3/bin/python\",\n",
        "  \"-m\",\n",
        "  \"ipykernel\",\n",
        "  \"-f\",\n",
        "  \"{connection_file}\"\n",
        " ],\n",
        " \"display_name\": \"Python 3.6 Singularity Tensorflow r1.12\",\n",
        " \"language\": \"python\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4IUTm3l7reQ",
        "colab_type": "code",
        "outputId": "02ca7f35-2f1e-44d7-b93d-2af1e7277b62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "import warnings\n",
        "\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    fxn()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rJOVmOW_LZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNdLD3y05Xuh",
        "colab_type": "text"
      },
      "source": [
        "- Tensorflow as started by Google, released in November 2015\n",
        "- Written in C++\n",
        "- Typically used from within Python directly or through the Keras module in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-NER2_YH7Dn",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow basics\n",
        "\n",
        "1. Define a directed graph\n",
        "2. execute\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/f/fe/Tred-G.svg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atwH1PalIXkm",
        "colab_type": "code",
        "outputId": "13c41206-ed9e-4079-83e9-cc692d5e0222",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(3, name=\"x\")\n",
        "y = tf.Variable(7, name=\"y\")\n",
        "f = x*100 + y*3 - 7\n",
        "print(f)\n",
        "with tf.Session() as sess:\n",
        "    x.initializer.run()\n",
        "    y.initializer.run()\n",
        "    result = f.eval()\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"sub:0\", shape=(), dtype=int32)\n",
            "314\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1YF5179I1-a",
        "colab_type": "text"
      },
      "source": [
        "# Keras\n",
        "- developed as a high-level interface to create neural networks with Tensorflow and Theano.\n",
        "- now it also supports Microsoft CNTK\n",
        "\n",
        "![alt text]( https://blynch.s3.msi.umn.edu/public/csci_5980_1_6.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiuhIP6pIKah",
        "colab_type": "code",
        "outputId": "1f8f6128-cd62-4577-9055-9099738b00d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 93us/sample - loss: 0.2860 - acc: 0.9172\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 4s 66us/sample - loss: 0.1411 - acc: 0.9582\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.1061 - acc: 0.9674\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0880 - acc: 0.9731\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0740 - acc: 0.9771\n",
            "10000/10000 [==============================] - 0s 47us/sample - loss: 0.0703 - acc: 0.9786\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07031472590686753, 0.9786]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CZ4QnOTILXd",
        "colab_type": "code",
        "outputId": "2251f728-4f55-40b1-bfc4-353033673b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttyGao1VJ5Xv",
        "colab_type": "code",
        "outputId": "3c5ef0ea-f20a-46b0-9fa3-c67e4fc6fb70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "!mkdir /content/scratch\n",
        "output_basename   = 'blynch-job1.hdf5'\n",
        "output_model_name = '/content/scratch/' + output_basename\n",
        "\n",
        "checkpointer = ModelCheckpoint(output_model_name, monitor='val_loss', verbose=1, mode='auto')\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, callbacks=[checkpointer])\n",
        "\n",
        "!ls -l /content/scratch\n",
        "\n",
        "\n",
        "#model = load_model(input_model_name)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "59712/60000 [============================>.] - ETA: 0s - loss: 0.0648 - acc: 0.9796\n",
            "Epoch 00001: saving model to /content/scratch/blynch-job1.hdf5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0648 - acc: 0.9796\n",
            "Epoch 2/5\n",
            "59648/60000 [============================>.] - ETA: 0s - loss: 0.0590 - acc: 0.9806\n",
            "Epoch 00002: saving model to /content/scratch/blynch-job1.hdf5\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0590 - acc: 0.9806\n",
            "Epoch 3/5\n",
            "59296/60000 [============================>.] - ETA: 0s - loss: 0.0524 - acc: 0.9825\n",
            "Epoch 00003: saving model to /content/scratch/blynch-job1.hdf5\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0528 - acc: 0.9823\n",
            "Epoch 4/5\n",
            "59584/60000 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9849\n",
            "Epoch 00004: saving model to /content/scratch/blynch-job1.hdf5\n",
            "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0469 - acc: 0.9849\n",
            "Epoch 5/5\n",
            "59328/60000 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9849\n",
            "Epoch 00005: saving model to /content/scratch/blynch-job1.hdf5\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0452 - acc: 0.9850\n",
            "total 1224\n",
            "-rw-r--r-- 1 root root 1249616 Feb 18 21:30 blynch-job1.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALIC80_pf_m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy a trained model to somewhere more permanent\n",
        "\n",
        "!cp /content/scratch/blynch-job1.hdf5 '/content/gdrive/My Drive/Tensorflow'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHOSNWenuBiu",
        "colab_type": "text"
      },
      "source": [
        "#TensorBoard\n",
        "- Tensorboard is a tool to vizualize TensorFlow graphs and output.\n",
        "- TensorFlow can output metrics to track the optimization process for a model\n",
        "\n",
        "```\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
        "```\n",
        "\n",
        "![alt text](https://www.tensorflow.org/tensorboard/images/tensorboard.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of5IqXU4vnEK",
        "colab_type": "text"
      },
      "source": [
        "# Stacking layers\n",
        "Instea of defining the entire model in 1 line, we can add layers like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0M0k-GHvmIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7eQZUHWwGXP",
        "colab_type": "text"
      },
      "source": [
        "### Then we can add layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg8k8Z0XwamA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(units=64, activation='relu', input_dim=100))\n",
        "model.add(Dense(units=10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ3Yzu-qwn-i",
        "colab_type": "text"
      },
      "source": [
        "and then compile the model with a loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJW2rhNSwxVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='sgd',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WFPQNT2xaYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWJDWwiHyFWt",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"Drawing\" width=\"800\"/>\n",
        "\n",
        "```\n",
        "  inputs = Input((IMG_WIDTH,IMG_WIDTH, 1))\n",
        "\n",
        "  layers = [0] * (U_DEPTH*4+2)\n",
        "  crops  = [0] *  U_DEPTH\n",
        "  layers[0] = Lambda(lambda x: x / 255) (inputs)\n",
        "  \n",
        "  for i in range(U_DEPTH):\n",
        "      features = MIN_FEATURES*2**i\n",
        "      layers[2*i+1] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[2*i])\n",
        "      layers[2*i+1] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[2*i+1])\n",
        "      layers[2*i+2] = MaxPooling2D((2, 2)) (layers[2*i+1])\n",
        "  features = MIN_FEATURES*2**U_DEPTH\n",
        "  layers[U_DEPTH*2+1] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[U_DEPTH*2])\n",
        "  layers[U_DEPTH*2+1] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[U_DEPTH*2+1])\n",
        "\n",
        "  for i in range(U_DEPTH):\n",
        "      edge = 2**(i+2) + 2**(i+3) - 2**3\n",
        "      features = MIN_FEATURES*2**(U_DEPTH-i-1)\n",
        "      crops[i] = Cropping2D((edge, edge))(layers[U_DEPTH*2-1-2*i])\n",
        "      layers[U_DEPTH*2+2+i*2] = Conv2DTranspose(features, (2, 2), strides=(2, 2), padding=padding) (layers[U_DEPTH*2+1+i*2])\n",
        "      layers[U_DEPTH*2+2+i*2] = concatenate([layers[U_DEPTH*2+2+i*2], crops[i]], axis=3)\n",
        "      layers[U_DEPTH*2+3+i*2] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[U_DEPTH*2+2+i*2])\n",
        "      layers[U_DEPTH*2+3+i*2] = Conv2D(features, (3, 3), activation='elu', kernel_initializer='he_normal', padding=padding) (layers[U_DEPTH*2+3+i*2])\n",
        "\n",
        "  outputs = Conv2D(1, (1, 1), activation='sigmoid') (layers[U_DEPTH*4+1])\n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hp3xA6CWMNhG",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLPcY1WW0YB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        ")\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "learning_rate = 1e-4\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "    # override the __call__ operator so you can call them like functions. When\n",
        "    # doing so you pass a Tensor of input data to the Module and it produces\n",
        "    # a Tensor of output data.\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "    # values of y, and the loss function returns a Tensor containing the\n",
        "    # loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Zero the gradients before running the backward pass.\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "    # parameters of the model. Internally, the parameters of each Module are stored\n",
        "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "    # all learnable parameters in the model.\n",
        "    loss.backward()\n",
        "\n",
        "    # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "    with torch.no_grad():\n",
        "        for param in model.parameters():\n",
        "            param -= learning_rate * param.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D6Mq27-NGq7",
        "colab_type": "text"
      },
      "source": [
        "# Using GPUs and Parallel Training\n",
        "\n",
        "## Tensorflow\n",
        "\n",
        "When using a GPU-enabled version of Tensorflow on hardware with GPUs, TensorFlow will try to make use of the hardware. This is a good place to start until you have a very thorough understanding of your problem. After that, you can look into:\n",
        "\n",
        "tf.distribute.Strategy - **use multiple GPUs** <br/>\n",
        "tf.distribute.Strategy - **use multiple NODEs**\n",
        "\n",
        "## Pytorch\n",
        "DataParallel - **multiple GPUs**\n",
        "torch.distributed - **multiple nodes**\n",
        "\n",
        "### Horovod\n",
        "Another framework for running Tensorflow or Pytorch over multiple nodes. \n",
        "\n",
        "```\n",
        " cp -r /home/dhp/public/deep_learning/horovad/use_8_gpus .\n",
        " cd use_8_gpus\n",
        " qsub pbs_run\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X198k1ZX1Uix",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}