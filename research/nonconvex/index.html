<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Ju Sun | Provable Nonconvex Methods/Algorithms</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/research/nonconvex/">
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Ju</strong> Sun
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">Welcome</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">Blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/pub/">Publications</a>
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/research/">Research</a>
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Provable Nonconvex Methods/Algorithms</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Provable Nonconvex Methods/Algorithms clearfix">
    <p>General nonconvex optimization is undoubtedly hard — in sharp contrast to convex optimization, of which there is good separation of problem structure, input data, and optimization algorithms. But many nonconvex problems of interest become amenable to simple and practical algorithms and rigorous analyses once the artificial separation is removed. This page collects recent research effort in this line. (<strong>Update: Apr 21 2018</strong>)</p>

<p>[<span style="color:red"><strong>S</strong></span>] indicates my contribution.</p>

<p>[<span style="color:red"><strong>New</strong></span>] A BibTex file for papers listed on the page can be downloaded <a href="/_pages/research/nonconvex/NCVX.bib">HERE</a>!</p>

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

<h2 id="contents">Contents</h2>

<ul>
  <li><a href="#review-articles">Review Articles</a></li>
  <li><a href="#problems-with-hidden-convexity-or-analytic-solutions">Problems with Hidden Convexity or Analytic Solutions</a>
    <ul>
      <li><a href="#blind-deconvolution">Blind Deconvolution</a></li>
      <li><a href="#separable-nonnegative-matrix-factorization-nmf">Separable Nonnegative Matrix Factorization (NMF)</a></li>
    </ul>
  </li>
  <li><a href="#problems-with-provable-global-results">Problems with Provable Global Results</a>
    <ul>
      <li><a href="#matrix-completionsensing">Matrix Completion/Sensing</a></li>
      <li><a href="#tensor-recoverydecomposition-hidden-variable-models">Tensor Recovery/Decomposition &amp; Hidden Variable Models</a></li>
      <li><a href="#phase-retrieval">Phase Retrieval</a></li>
      <li><a href="#dictionary-learning">Dictionary Learning</a></li>
      <li><a href="#deep-learning">Deep Learning</a></li>
      <li><a href="#sparse-vectors-in-linear-subspaces">Sparse Vectors in Linear Subspaces</a></li>
      <li><a href="#nonnegativesparse-principal-component-analysis">Nonnegative/Sparse Principal Component Analysis</a></li>
      <li><a href="#mixed-linear-regression">Mixed Linear Regression</a></li>
      <li><a href="#blind-deconvolutioncalibration">Blind Deconvolution/Calibration</a></li>
      <li><a href="#super-resolution">Super Resolution</a></li>
      <li><a href="#synchronization-problemscommunity-detection">Synchronization Problems/Community Detection</a></li>
      <li><a href="#joint-alignment">Joint Alignment</a></li>
      <li><a href="#numerical-linear-algebra">Numerical Linear Algebra</a></li>
      <li><a href="#bayesian-inference">Bayesian Inference</a></li>
      <li><a href="#empirical-risk-minimization-shallow-networks">Empirical Risk Minimization &amp; Shallow Networks</a></li>
      <li><a href="#system-identification">System Identification</a></li>
      <li><a href="#burer-monteiro-style-decomposition-algorithms">Burer-Monteiro Style Decomposition Algorithms</a></li>
      <li><a href="#generic-structured-problems">Generic Structured Problems</a></li>
      <li><a href="#nonconvex-feasibility-problems">Nonconvex Feasibility Problems</a></li>
    </ul>
  </li>
  <li><a href="#of-statistical-nature-">Of Statistical Nature …</a></li>
  <li><a href="#relevant-optimization-methodstheory-miscs">Relevant Optimization Methods/Theory &amp; Miscs</a></li>
</ul>

<!-- /TOC -->

<h2 id="review-articles">Review Articles</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1802.08397">Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.07897">Non-convex Optimization for Machine Learning</a> (2017)</li>
</ul>

<h2 id="problems-with-hidden-convexity-or-analytic-solutions">Problems with Hidden Convexity or Analytic Solutions</h2>
<ul>
  <li><a href="http://www.stat.cmu.edu/~ryantibs/convexopt/lectures/nonconvex.pdf">These slides</a> summarize lots of them.</li>
</ul>

<h3 id="blind-deconvolution">Blind Deconvolution</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1211.5608">Blind Deconvolution using Convex Programming</a> (2012)</li>
</ul>

<h3 id="separable-nonnegative-matrix-factorization-nmf">Separable Nonnegative Matrix Factorization (NMF)</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1507.02189">Intersecting Faces: Non-negative Matrix Factorization With New Guarantees</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1401.5226">The why and how of nonnegative matrix factorization</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1111.0952">Computing a Nonnegative Matrix Factorization – Provably</a> (2011)</li>
</ul>

<h2 id="problems-with-provable-global-results">Problems with Provable Global Results</h2>

<h3 id="matrix-completionsensing">Matrix Completion/Sensing</h3>
<p>(See also <a href="/research/low-rank/">low-rank matrix/tensor recovery</a> )</p>

<ul>
  <li><a href="https://arxiv.org/abs/1803.07554">The Leave-one-out Approach for Matrix Completion: Primal and Dual Analysis</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06286">Nonconvex Matrix Factorization from Rank-One Measurements</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.09203">Algorithmic Regularization in Over-parameterized Matrix Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.05519">Accelerated Alternating Projections for Robust Principal Component Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02524">Provable quantum state tomography via non-convex methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01742">Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.08114">Nonconvex Low-Rank Matrix Recovery with Arbitrary Outliers via Median-Truncated Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.00257">Robust Principal Component Analysis by Manifold Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.09726">Spectral Compressed Sensing via Projected Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.08934">Reexamining Low Rank Matrix Factorization for Trace Norm Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03896">A Well-Tempered Landscape for Non-convex Robust Subspace Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.08683">Optimal Sample Complexity for Matrix Completion and Related Problems via $\ell_2$-Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.01265">Geometry of Factored Nuclear Norm Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.00708">No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.09848">Painless Breakups – Efficient Demixing of Low Rank Matrices</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08651">Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimizations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.07945">Global Optimality in Low-rank Matrix Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06525">A Nonconvex Free Lunch for Low-Rank plus Sparse Matrix Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1612.09296">Symmetry, Saddle Points, and Global Geometry of Nonconvex Matrix Factorization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.03240">Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.07315">Nearly-optimal Robust Matrix Completion</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.01316">Provable non-convex projected gradient descent for a class of constrained matrix optimization problems</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.03168">Finding Low-rank Solutions to Matrix Problems, Efficiently and Provably</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.08370">Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07784">Fast Algorithms for Robust PCA via Gradient Descent</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07051">Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.07272">Matrix Completion has No Spurious Local Minimum</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.07221">Global Optimality of Local Search for Low Rank Matrix Recovery</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.06610">Guarantees of Riemannian Optimization for Low Rank Matrix Completion</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.02164">A Note on Alternating Minimization Algorithm for the Matrix Completion Problem</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.02262">Recovery guarantee of weighted low-rank approximation via alternating minimization</a> (2016)</li>
  <li><a href="http://dx.doi.org/10.1007/978-3-319-24486-0_1">Efficient Matrix Sensing Using Rank-1 Gaussian Measurements</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1511.01562">Guarantees of Riemannian Optimization for Low Rank Matrix Recovery</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1509.03025">Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1507.03566">Low-rank Solutions of Linear Matrix Equations via Procrustes Flow</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1506.06081">A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1411.8003">Guaranteed Matrix Completion via Non-convex Factorization</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1411.1087">Fast Exact Matrix Completion with Finite Samples</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1410.7660">Non-convex Robust PCA</a> (2014)</li>
  <li><a href="http://jmlr.org/proceedings/papers/v35/hardt14a.pdf">Fast Matrix Completion without the Condition Number</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1312.0925">Understanding Alternating Minimization for Matrix Completion</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1212.0467">Low-rank Matrix Completion using Alternating Minimization</a> (2012)</li>
  <li><a href="http://arxiv.org/abs/0901.3150">Matrix Completion from a Few Entries</a> (2009)</li>
  <li><a href="http://arxiv.org/abs/0909.5457">Guaranteed Rank Minimization via Singular Value Projection</a> (2009)</li>
</ul>

<h3 id="tensor-recoverydecomposition--hidden-variable-models">Tensor Recovery/Decomposition &amp; Hidden Variable Models</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1801.09326">Sparse and Low-rank Tensor Estimation via Cubic Sketchings</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1711.05424">The landscape of the spiked tensor model</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.04934">Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.05598">On the Optimization Landscape of Tensor Decompositions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.02724">Tensor SVD: Statistical and Computational Limits</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.01804">Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06980">On Polynomial Time Methods for Exact Low Rank Tensor Completion</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1610.09322">Homotopy Method for Tensor PCA</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.01690">Low-tubal-rank Tensor Completion using Alternating Minimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1512.02337">Speeding up sum-of-squares for tensor decomposition and planted sparse vectors</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1510.04747">Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1411.1488">Analyzing Tensor Power Method Dynamics: Applications to Learning Overcomplete Latent Variable Models</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1210.7559">Tensor decompositions for learning latent variable models</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.0553">Provable Learning of Overcomplete Latent Variable Models: Semi-supervised and Unsupervised Settings</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1406.2784">Provable Tensor Factorization with Missing Data</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1402.5180">Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates</a> (2014)</li>
</ul>

<h3 id="phase-retrieval">Phase Retrieval</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1803.07726">Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.01170">Optimization-based AMP for Phase Retrieval: The Impact of Initialization and $\ell_2$-regularization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.03278">Compressive Phase Retrieval of Structured Signal</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.06245">Misspecified Nonconvex Statistical Optimization for Phase Retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.02426">Compressive Phase Retrieval via Reweighted Amplitude Flow</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.02083">A Local Analysis of Block Coordinate Descent for Gaussian Phase Retrieval</a> ([<span style="color:red"><strong>S</strong></span>], 2017)</li>
  <li><a href="https://arxiv.org/abs/1712.00716">Convolutional Phase Retrieval via Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.01712">Linear Convergence of An Iterative Phase Retrieval Algorithm with Data Reuse</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03247">The nonsmooth landscape of phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05234">Phase Retrieval via Linear Programming: Fundamental Limits and Algorithmic Improvements</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.10291">Convergence of the randomized Kaczmarz method for phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.09993">Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.08167">Phase retrieval using alternating minimization in a batch setting</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.10407">Solving Almost all Systems of Random Quadratic Equations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.06412">Phase Retrieval Using Structured Sparsity: A Sample Efficient Algorithmic Framework</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02356">Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.06256">Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.03286">Phase Retrieval via Sparse Wirtinger Flow</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06175">Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.07641">Sparse Phase Retrieval via Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.09540">Solving Large-scale Systems of Random Quadratic Equations via Stochastic Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1608.04141">Low Rank Phase Retrieval</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.03088">Phase retrieval with random Gaussian sensing vectors by alternating projections</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1607.08218">Non-Convex Phase Retrieval from STFT Measurements</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.08135">Gauss-Newton Method for Phase Retrieval</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.03196">Phase Retrieval via Incremental Truncated Wirtinger Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.08285">Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07719">Reshaped Wirtinger Flow for Solving Quadratic Systems of Equations</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.03805">Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.06664">A Geometric Analysis of Phase Retrieval</a> ([<span style="color:red"><strong>S</strong></span>], 2016)</li>
  <li><a href="http://dx.doi.org/10.1109/TIT.2017.2672727">Phase retrieval for wavelet transforms </a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1506.07868">The Local Convexity of Solving Quadratic Equations</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1502.01822">Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1505.05114">Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1407.1065">Phase Retrieval via Wirtinger Flow: Theory and Algorithms</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1306.0160">Phase Retrieval using Alternating Minimization</a> (2013)</li>
</ul>

<h3 id="dictionary-learning">Dictionary Learning</h3>
<p>(See also <strong>Theory</strong> part in <a href="/research/dict-learn/">Dictionary/Deep Learning</a>)</p>

<ul>
  <li><a href="https://arxiv.org/abs/1711.03638">A Provable Approach for Double-Sparse Coding</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03634">Alternating minimization for dictionary learning with random initialization</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1504.06785">Complete Dictionary Recovery over the Sphere</a> ([<span style="color:red"><strong>S</strong></span>], 2015)</li>
  <li><a href="http://arxiv.org/abs/1503.00778">Simple, Efficient, and Neural Algorithms for Sparse Coding</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1401.0579">More Algorithms for Provable Dictionary Learning</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1309.1952">Exact Recovery of Sparsely Used Overcomplete Dictionaries</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1308.6273">New Algorithms for Learning Incoherent and Overcomplete Dictionaries</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1310.7991">Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization</a> (2013)</li>
</ul>

<h3 id="deep-learning">Deep Learning</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1803.00909">Understanding the Loss Surface of Neural Networks for Binary Classification</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06093">Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.01473">Deep linear neural networks with arbitrary loss: All local minima are global</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.02444">Global optimality conditions for deep neural networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08580">Depth Creates No Bad Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.00458">Electron-Proton Dynamics in Deep Learning</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1605.07110">Deep Learning without Poor Local Minima</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.08361">No bad local minima: Data independent training error guarantees for multilayer neural networks</a> (2016)</li>
</ul>

<h3 id="sparse-vectors-in-linear-subspaces">Sparse Vectors in Linear Subspaces</h3>
<p>(See <a href="/research/struct-elem/">Structured Element Pursuit</a> )</p>

<h3 id="nonnegativesparse-principal-component-analysis">Nonnegative/Sparse Principal Component Analysis</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1611.03819">Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1406.4775">Non-negative Principal Component Analysis: Message Passing Algorithms and Sharp Asymptotics</a> (2014)</li>
</ul>

<h3 id="mixed-linear-regression">Mixed Linear Regression</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1608.05749">Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1412.3046">Provable Tensor Methods for Learning Mixtures of Classifiers</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1310.3745">Alternating Minimization for Mixed Linear Regression</a> (2013)</li>
</ul>

<h3 id="blind-deconvolutioncalibration">Blind Deconvolution/Calibration</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1712.00111">Blind Gain and Phase Calibration via Sparse Spectral Methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.03309">Blind Deconvolution by a Steepest Descent Algorithm on a Quotient Manifold</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.08642">Regularized Gradient Descent: A Nonconvex Recipe for Fast Joint Blind Deconvolution and Demixing</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.04196">Self-Calibration via Linear Least Squares</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.09028">Through the Haze: A Non-Convex Approach to Blind Calibration for Linear Random Sensing Models</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.06469">Fast and guaranteed blind multichannel deconvolution under a bilinear system model</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.04933">Rapid, Robust, and Reliable Blind Deconvolution via Nonconvex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.02615">A Non-Convex Blind Calibration Method for Randomised Sensing Strategies</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1511.06146">RIP-like Properties in Subsampled Blind Deconvolution</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1511.06149">Blind Recovery of Sparse Signals from Subsampled Convolution</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1312.0525">Near Optimal Compressed Sensing of Sparse Rank-One Matrices via Sparse Power Factorization</a> (2013)</li>
</ul>

<h3 id="super-resolution">Super Resolution</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1511.03385">Greed is Super: A Fast Algorithm for Super-Resolution</a> (2015)</li>
</ul>

<h3 id="synchronization-problemscommunity-detection">Synchronization Problems/Community Detection</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1703.06857">Near-optimal bounds for phase synchronization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1610.04583">Message-passing algorithms for synchronization problems over compact groups</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04426">On the low-rank approach for semidefinite programs arising in synchronization and community detection</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1601.06114">Nonconvex phase synchronization</a> (2016)</li>
</ul>

<h3 id="joint-alignment">Joint Alignment</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1609.05820">The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences</a> (2016)</li>
</ul>

<h3 id="numerical-linear-algebra">Numerical Linear Algebra</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1803.04049">PCA by Determinant Optimization has no Spurious Local Optima</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1605.01036">Orbital minimization method with $\ell_1$ regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.01256">The Global Optimization Geometry of Nonsymmetric Matrix Factorization and Sensing</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1507.08366">On the matrix square root via geometric optimization</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1507.05854">Computing Matrix Squareroot via Non Convex Local Search</a> (2015)</li>
</ul>

<h3 id="bayesian-inference">Bayesian Inference</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1503.06567">On some provably correct cases of variational inference for topic models</a> (2015)</li>
</ul>

<h3 id="empirical-risk-minimization--shallow-networks">Empirical Risk Minimization &amp; Shallow Networks</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1804.06561">A Mean Field View of the Landscape of Two-Layers Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.05012">Representing smooth functions as compositions of near-identity functions with implications for deep network optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.09319">SUNLayer: Stable denoising with generative networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.04304">Representation Learning and Recovery in the ReLU Model</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.02968">Learning Deep Models: Critical Points and Local Openness</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.01206">On the Power of Over-parametrization in Neural Networks with Quadratic Activation</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07301">On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06463">Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.03487">A Critical View of Global Optimality in Deep Learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.10132">The Multilinear Structure of ReLU Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.08968">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.00779">Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03440">Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.00501">Learning One-hidden-layer Neural Networks with Landscape Design</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11241">Theoretical properties of the global optimizer of two layer neural network</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11205">Critical Points of Neural Networks: Analytical Forms and Landscape Properties</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10174">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.02196">Porcupine Neural Networks: (Almost) All Local Optima are Global</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.06129">When is a Convolutional Filter Easy To Learn?</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.04926">Theoretical insights into the optimization landscape of over-parameterized shallow neural networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03175">Recovery Guarantees for One-hidden-layer Neural Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.09886">Convergence Analysis of Two-layer Neural Networks with ReLU Activation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.07576">Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.04591">Learning ReLUs via Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.07966">Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1609.05191">Gradient Descent Learns Linear Dynamical Systems</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1607.06534">The Landscape of Empirical Risk for Non-convex Losses</a> (2016)</li>
</ul>

<h3 id="system-identification">System Identification</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1609.05191">Gradient Descent Learns Linear Dynamical Systems</a> (2016)</li>
</ul>

<h3 id="burer-monteiro-style-decomposition-algorithms">Burer-Monteiro Style Decomposition Algorithms</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1804.02008">Deterministic guarantees for Burer-Monteiro factorizations of smooth semidefinite programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.00186">Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1703.08729">Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.03060">The Nonconvex Geometry of Low-Rank Matrix Optimizations with General Objective Functions</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.04970">The non-convex Burer-Monteiro approach works on smooth semidefinite programs</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04426">On the low-rank approach for semidefinite programs arising in synchronization and community detection</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1411.1134">Global Convergence of Stochastic Gradient Descent for Some Nonconvex Matrix Problems</a> (2014)</li>
</ul>

<h3 id="generic-structured-problems">Generic Structured Problems</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1711.08172">Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1711.02621">Convex Optimization with Nonconvex Oracles</a> (2017)</li>
</ul>

<h3 id="nonconvex-feasibility-problems">Nonconvex Feasibility Problems</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1709.05984">A convergent relaxation of the Douglas-Rachford algorithm</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.04846">A Lyapunov-type approach to convergence of the Douglas-Rachford algorithm</a> (2017)</li>
  <li><a href="https://carma.newcastle.edu.au/DRmethods/">Feasibility Problems: Douglas-Rachford and Projection Methods</a> (Project Page)</li>
  <li><a href="https://arxiv.org/abs/1610.03975">Dynamics of the Douglas-Rachford Method for Ellipses and p-Spheres</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1609.08751">A remark on the convergence of the Douglas-Rachford iteration in a non-convex setting</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1604.04657">On the finite convergence of the Douglas-Rachford algorithm for solving (not necessarily convex) feasibility problems in Euclidean spaces</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1506.09026">Global Behavior of the Douglas-Rachford Method for a Nonconvex Feasibility Problem</a> (2015)</li>
  <li><a href="http://dx.doi.org/10.1007/978-1-4419-9569-8_6">The Douglas–Rachford Algorithm in the Absence of Convexity</a> (2011)</li>
</ul>

<h2 id="of-statistical-nature-">Of Statistical Nature …</h2>

<ul>
  <li><a href="http://arxiv.org/abs/1609.04522">Sparse Tensor Graphical Model: Non-convex Optimization and Statistical Inference</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1512.08269">Statistical and Computational Guarantees for the Baum-Welch Algorithm</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1502.01425">Provable Sparse Tensor Decomposition</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1501.00312">Statistical consistency and asymptotic normality for high-dimensional robust M-estimators</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1412.5632">Support recovery without incoherence: A case for nonconvex regularizations</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1412.8729">High Dimensional Expectation-Maximization Algorithm: Statistical Optimization and Asymptotic Normality</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.2156">Statistical guarantees for the EM algorithm: From population to sample-based analysis</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.5352">Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1305.2436">Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local optima</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1109.3714">High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity</a> (2011)</li>
</ul>

<h2 id="relevant-optimization-methodstheory--miscs">Relevant Optimization Methods/Theory &amp; Miscs</h2>
<ul>
  <li><a href="https://arxiv.org/abs/1804.02907">On the spherical quasi-convexity of quadratic functions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.01076">Operator Scaling via Geodesically Convex Optimization, Invariant Theory and Polynomial Identity Testing</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.08600">Lower error bounds for the stochastic gradient descent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning rates</a> (2018)</li>
  <li><a href="https://doi.org/10.1137/17M1127582">A Riemannian BFGS Method Without Differentiated Retraction for Nonconvex Optimization Problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.03943">Nonconvex weak sharp minima on Riemannian manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.02924">A Newton-CG Algorithm with Complexity Guarantees for Smooth Unconstrained Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07796">Continuous Relaxation of MAP Inference: A Nonconvex Perspective</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.10418">On the Sublinear Convergence of Randomly Perturbed Alternating Gradient Descent to Second Order Stationary Solutions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.09592">ADMM for Multiaffine Constrained Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.09128">Averaging Stochastic Gradient Descent on Riemannian Manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.08941">Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solutions for Nonconvex Distributed Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07843">Concise Complexity Analyses for Trust-Region Methods</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02988">Stochastic subgradient method converges at the rate $O(k^{-1/4})$ on weakly convex functions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06509">On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.08246">Characterizing Implicit Bias in Terms of Optimization Geometry</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06439">Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06384">Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07372">Sample Complexity of Stochastic Variance-Reduced Cubic Regularization for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.05155">Toward Deeper Understanding of Nonconvex Stochastic Optimization with Momentum using Diffusion Approximations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.03889">Convergence Analysis of Alternating Nonconvex Projections</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02628">Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02688">Exact Semidefinite Formulations for a Class of (Random and Non-Random) Nonconvex Quadratic Programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.01062">How to Characterize the Worst-Case Performance of Algorithms for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.09387">On the Quadratic Convergence of the Cubic Regularization Method under a Local Error Bound Condition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.01994">The proximal alternating direction method of multipliers in the nonconvex setting: convergence analysis and rates</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.03013">Nonconvex Lagrangian-Based Optimization: Monitoring Schemes and Global Convergence</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.06585">Third-order Smoothness Helps: Even Faster Stochastic Optimization Algorithms for Finding Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.03950">Saving Gradient and Negative Curvature Computations: Finding Local Minima More Efficiently</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.01033">NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.10456">Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.10467">Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.06673">Neon2: Finding Local Minima via First-Order Oracles</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.08172">Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.05224">Revisiting Normalized Gradient Descent: Evasion of Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02838">Stochastic Cubic Regularization for Fast Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02621">Convex Optimization with Nonconvex Oracles</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01944">First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01303">On local non-global minimizers of quadratic functions with cubic regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10770">Frank-Wolfe methods for geodesically convex optimization with application to the matrix geometric mean</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10329">Lower Bounds for Higher-Order Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.00841">Lower Bounds for Finding Stationary Points II: First-Order Methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11606">Lower Bounds for Finding Stationary Points I</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.09447">Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.09047">Block Coordinate Descent Only Converge to Minimizers</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.07406">First-order Methods Almost Always Avoid Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05338">Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05017">The power of sum-of-squares for detecting hidden structures</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.02236">Primal-Dual Optimization Algorithms over Riemannian Manifolds: an Iteration Complexity Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.08571">On Noisy Negative Curvature Descent: Competing with Gradient Descent for Faster Non-convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.07180">Worst-case evaluation complexity and optimality of second-order methods for nonconvex smooth optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.05747">Douglas-Rachford splitting and ADMM for nonconvex optimization: new convergence results and accelerated versions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.05758">Local Minimizers and Second-Order Conditions in Composite Piecewise Programming via Directional Derivatives</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.04451">Alternating minimization and alternating descent over nonconvex sets</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.07827">Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.07164">Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.04783">Non-convex Conditional Gradient Sliding</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.04044">Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.00475">An Inexact Regularized Newton Framework with a Worst-Case Iteration Complexity of $O(\varepsilon^{-3/2})$ for Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.08028">A Second Order Method for Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.07993">Behavior of Accelerated Gradient Methods Near Critical Points of Nonconvex Problems</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.05681">Mirror descent in non-convex stochastic programming</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03131">Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.04097">Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03267">An Alternative to EM for Gaussian Mixture Models: Batch and Stochastic Riemannian Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.00896">Using Negative Curvature in Solving Nonlinear Programs</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.10412">Gradient Descent Can Take Exponential Time to Escape Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.07285">Optimality of orders one to three and beyond: characterization and evaluation complexity in constrained nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.05933">Sub-sampled Cubic Regularization for Non-convex Optimization</a> (2017)</li>
  <li><a href="https://doi.org/10.1007/s10957-017-1093-4">Iteration-Complexity of Gradient, Subgradient and Proximal Point Methods on Riemannian Manifolds</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02502">Linearized ADMM for Non-convex Non-smooth Optimization with Convergence Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02766">“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.08227">Accelerating Stochastic Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.04548">On the Gap Between Strict-Saddles and True Convexity: An $\Omega(\log d)$ Lower Bound for Eigenvector Approximation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.10993">Catalyst Acceleration for Gradient-Based Non-Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.07915">Perspective: Energy Landscapes for Machine Learning</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.07755">Gradient descent with nonconvex constraints: local concavity determines convergence</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.04890">Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00887">How to Escape Saddle Points Efficiently</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00329">Convergence rate of a simulated annealing algorithm with noisy observations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00412">Exploiting Negative Curvature in Deterministic and Stochastic Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08134">Online Multiview Representation Learning: Dropping Convexity for Better Efficiency</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.00763">Natasha: Faster Stochastic Non-Convex Optimization via Strongly Non-Convex Parameter</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06435">Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1701.06501">Maximum likelihood estimation of determinantal point processes</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1701.04271">Fast Rates for Empirical Risk Minimization of Strict Saddle Problems</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1612.00547">Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.04831">The Power of Normalization: Faster Evasion of Saddle Points</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.00756">Accelerated Methods for Non-Convex Optimization</a> (2016)</li>
  <li><a href="https://dx.doi.org/10.1007/s10107-016-1065-8">Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.01146">Finding Local Minima for Nonconvex Optimization in Linear Time</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1609.07428">Convergence Rate Analysis of a Stochastic Trust Region Method for Nonconvex Optimization</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1608.07630">Global analysis of Expectation Maximization for mixtures of two Gaussians</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.00978">Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.09070">Local Convergence of the Heavy-ball Method and iPiano for Non-convex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.08101">Global rates of convergence for nonconvex optimization on manifolds</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.00405">Gradient Descent Converges to Minimizers: The Case of Non-Isolated Critical Points</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1604.04603">On the Douglas-Rachford algorithm</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.04064">A Grothendieck-type inequality for local maxima</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.06053">First-order Methods for Geodesically Convex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.05908">Efficient approaches for escaping higher order saddle points in non-convex optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04915">Gradient Descent Converges to Minimizers</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1510.06096">When Are Nonconvex Problems Not Scary?</a> ([<span style="color:red"><strong>S</strong></span>], 2015; see also <a href="/docs/thesis.pdf">my thesis</a>)</li>
  <li><a href="http://www.optimization-online.org/DB_HTML/2014/11/4635.html">On the Global Optimality for Linear Constrained Rank Minimization Problem</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1511.06324">Global Convergence of ADMM in Nonconvex Nonsmooth Optimization</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1509.03917">Dropping Convexity for Faster Semi-definite Optimization</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1508.04468">Local Linear Convergence of the ADMM/Douglas–Rachford Algorithms without Strong Convexity and Application to Statistical Imaging</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1501.07242">Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1503.02101">Escaping From Saddle Points — Online Stochastic Gradient for Tensor Decomposition</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1507.00887">Peaceman-Rachford splitting for a class of nonconvex optimization problems</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1409.8444">Douglas-Rachford splitting for nonconvex optimization with application to nonconvex feasibility problems</a> (2014)</li>
  <li><a href="https://arxiv.org/abs/1407.0753">Global convergence of splitting methods for nonconvex composite optimization</a> (2014)</li>
  <li><a href="https://arxiv.org/abs/1310.3787">Accelerated Gradient Methods for Nonconvex Nonlinear and Stochastic Programming</a> (2013)</li>
  <li><a href="http://dx.doi.org/10.1007/s10107-013-0701-9">Proximal alternating linearized minimization for nonconvex and nonsmooth problems</a> (2013)</li>
  <li><a href="http://dx.doi.org/10.1007/s10107-011-0484-9">Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward–backward splitting, and regularized Gauss–Seidel methods</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/0801.1780">Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequality</a> (2008)</li>
  <li><a href="https://dx.doi.org/10.1007/s10107-006-0706-8">Cubic regularization of Newton method and its global performance</a> (2006)</li>
  <li><a href="http://dx.doi.org/10.1137/030602290">Computing the Local-Nonglobal Minimizer of a Large Scale Trust-Region Subproblem</a> (2005)</li>
  <li><a href="http://dx.doi.org/10.1137/S1052623494278049">On Some Properties of Quadratic Programs with a Convex Quadratic Constraint</a> (1998)</li>
  <li><a href="http://dx.doi.org/10.1137/0804009">Local minimizers of quadratic functions on Euclidean balls and spheres</a> (1994)</li>
</ul>

<blockquote>
  <p><strong>Disclaimer</strong> - This page is meant to serve a hub for references on this problem, and does not represent in any way personal endorsement of papers listed here. So I do not hold any responsibility for quality and technical correctness of each paper listed here. The reader is advised to use this resource with discretion.</p>
</blockquote>

<blockquote>
  <p><strong>If you’d like your paper to be listed here</strong>  - Just drop me a few lines via email (which can be found on “Welcome” page). If you don’t bother to spend a word, just deposit your paper on arXiv. I get email alert about new animals there every morning,  and will be happy to hunt one for this zoo if it seems <strong>fit</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Special thanks to</strong>: <a href="https://people.orie.cornell.edu/dsd95/">Damek Davis</a>, <a href="http://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a></p>
</blockquote>


  </article>

  

<!--  -->

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2018 Ju Sun.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!--load Ubuntu webfont -->

<link href='http://fonts.googleapis.com/css?family=Ubuntu|Ubuntu+Condensed|Ubuntu+Mono' rel='stylesheet' type='text/css'>

<!-- mathjax config-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true, 
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
    TeX: { equationNumbers: { autoNumber: "AMS", useLabelIds: true } }, 
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
