<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>Ju Sun | Provable Nonconvex Methods/Algorithms</title>
  <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/research/nonconvex/">
  
  <style>
	ul {list-style-type: circle;}
  </style>
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    
    <span class="site-title">
        
        <strong>Ju Sun</strong>
    </span>
    

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <!-- About -->
        <a class="page-link" href="/">Welcome</a>

        <!-- Blog -->
        <a class="page-link" href="/blog/">Blog</a>

        <!-- Pages -->
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/teach/">Teaching</a>
          
        
          
            <a class="page-link" href="/pub/">Publications</a>
          
        
          
            <a class="page-link" href="/talks/">Talks</a>
          
        
          
            <a class="page-link" href="/software/">Software</a>
          
        
          
            <a class="page-link" href="/grants/">Grants</a>
          
        
          
            <a class="page-link" href="/people/">People</a>
          
        
          
        
          
        
          
        
          
            <a class="page-link" href="/research/">Research</a>
          
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/CV.pdf">vitae</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Provable Nonconvex Methods/Algorithms</h1>
    <h5 class="post-description"></h5>
  </header>

  <article class="post-content Provable Nonconvex Methods/Algorithms clearfix">
    <p>General nonconvex optimization is undoubtedly hard — in sharp contrast to convex optimization, of which there is good separation of problem structure, input data, and optimization algorithms. But many nonconvex problems of interest become amenable to simple and practical algorithms and rigorous analyses once the artificial separation is removed. This page collects recent research effort in this line. (<strong>Update: Dec 11 2021</strong>)</p>

<p>[<span style="color:red"><strong>S</strong></span>] indicates my contribution.</p>

<p>[<span style="color:red"><strong>New</strong></span>] A BibTex file for papers listed on the page can be downloaded <a href="/research/nonconvex/NCVX.bib">HERE</a>!</p>

<h2 id="contents">Contents</h2>

<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

<ul>
  <li><a href="#contents">Contents</a></li>
  <li><a href="#review-articles">Review Articles</a></li>
  <li><a href="#software">Software</a></li>
  <li><a href="#problems-with-hidden-convexity-or-analytic-solutions">Problems with Hidden Convexity or Analytic Solutions</a>
    <ul>
      <li><a href="#blind-deconvolution">Blind Deconvolution</a></li>
      <li><a href="#separable-nonnegative-matrix-factorization-nmf">Separable Nonnegative Matrix Factorization (NMF)</a></li>
    </ul>
  </li>
  <li><a href="#problems-with-provable-global-results">Problems with Provable Global Results</a>
    <ul>
      <li><a href="#matrix-completionsensing">Matrix Completion/Sensing</a></li>
      <li><a href="#tensor-recoverydecomposition--hidden-variable-models">Tensor Recovery/Decomposition &amp; Hidden Variable Models</a></li>
      <li><a href="#phase-retrieval">Phase Retrieval</a></li>
      <li><a href="#dictionary-learning">Dictionary Learning</a></li>
      <li><a href="#deep-learning">Deep Learning</a></li>
      <li><a href="#sparse-vectors-in-linear-subspaces">Sparse Vectors in Linear Subspaces</a></li>
      <li><a href="#nonnegativesparse-principal-component-analysis">Nonnegative/Sparse Principal Component Analysis</a></li>
      <li><a href="#mixed-linear-regression">Mixed Linear Regression</a></li>
      <li><a href="#blind-deconvolutioncalibration">Blind Deconvolution/Calibration</a></li>
      <li><a href="#super-resolution">Super Resolution</a></li>
      <li><a href="#synchronization-problemscommunity-detection">Synchronization Problems/Community Detection</a></li>
      <li><a href="#joint-alignment">Joint Alignment</a></li>
      <li><a href="#numerical-linear-algebra">Numerical Linear Algebra</a></li>
      <li><a href="#bayesian-inference">Bayesian Inference</a></li>
      <li><a href="#empirical-risk-minimization--shallow-networks">Empirical Risk Minimization &amp; Shallow Networks</a></li>
      <li><a href="#system-identification">System Identification</a></li>
      <li><a href="#burer-monteiro-style-decomposition-algorithms">Burer-Monteiro Style Decomposition Algorithms</a></li>
      <li><a href="#generic-structured-problems">Generic Structured Problems</a></li>
      <li><a href="#nonconvex-feasibility-problems">Nonconvex Feasibility Problems</a></li>
    </ul>
  </li>
  <li><a href="#of-statistical-nature-">Of Statistical Nature …</a></li>
  <li><a href="#relevant-optimization-methods-theory-miscs">Relevant Optimization Methods, Theory, Miscs</a></li>
</ul>

<!-- /TOC -->

<h2 id="review-articles">Review Articles</h2>
<ul>
  <li><a href="https://arxiv.org/abs/2012.06188">Recent Theoretical Advances in Non-Convex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.06753">From Symmetry to Geometry: Tractable Nonconvex Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1809.09573">Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.08397">Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.07897">Non-convex Optimization for Machine Learning</a> (2017)</li>
</ul>

<h2 id="software">Software</h2>
<ul>
  <li><a href="https://ncvx.org/">NCVX–a general-purpose optimization package for nonconvex, particularly constrained and nonsmooth, problems</a> (see also <a href="http://www.timmitchell.com/software/GRANSO/">GRANSO</a>)</li>
  <li><a href="https://www.manopt.org/">Manopt–differentiable nonconvex problems with manifold constraints</a> (see also <a href="https://pymanopt.org/">PyManopt</a>)</li>
  <li><a href="http://www.geno-project.org/">GENO–Generic Optimization for Classical Machine Learning</a></li>
  <li><a href="https://pytorch.org/">Pytorch</a> and <a href="https://www.tensorflow.org/">Tensorflow</a> and <a href="https://github.com/google/jax">Jax</a>: Unconstrained optimization with auto-differentiation and GPU/TPU support</li>
</ul>

<h2 id="problems-with-hidden-convexity-or-analytic-solutions">Problems with Hidden Convexity or Analytic Solutions</h2>
<ul>
  <li><a href="http://www.stat.cmu.edu/~ryantibs/convexopt-F16/lectures/nonconvex.pdf">These slides</a> summarize lots of them.</li>
</ul>

<h3 id="blind-deconvolution">Blind Deconvolution</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1211.5608">Blind Deconvolution using Convex Programming</a> (2012)</li>
</ul>

<h3 id="separable-nonnegative-matrix-factorization-nmf">Separable Nonnegative Matrix Factorization (NMF)</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1507.02189">Intersecting Faces: Non-negative Matrix Factorization With New Guarantees</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1401.5226">The why and how of nonnegative matrix factorization</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1111.0952">Computing a Nonnegative Matrix Factorization – Provably</a> (2011)</li>
</ul>

<h2 id="problems-with-provable-global-results">Problems with Provable Global Results</h2>

<h3 id="matrix-completionsensing">Matrix Completion/Sensing</h3>
<p>(See also <a href="/research/low-rank/">low-rank matrix/tensor recovery</a> )</p>

<ul>
  <li><a href="https://arxiv.org/abs/2106.15013">Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2106.14289">Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2106.02119">A Scalable Second Order Method for Ill-Conditioned Matrix Completion from Few Samples</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.02969">Implicit Regularization of Sub-Gradient Method in Robust Matrix Recovery: Don’t be Afraid of Outliers</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.02756">On the computational and statistical complexity of over-parameterized matrix sensing</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.00937">Riemannian Perspective on Matrix Factorization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2011.13772">Gradient Descent for Deep Matrix Factorization: Dynamics and Implicit Bias towards Low Rank</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.08898">Accelerating Ill-Conditioned Low-Rank Matrix Estimation via Scaled Gradient Descent</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.13364">Low-Rank Matrix Recovery with Scaled Subgradient Methods: Fast and Robust Convergence Without the Condition Number</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.10395">Mixed-Projection Conic Optimization: A New Paradigm for Modeling Rank Constraints</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.12091">Implicit Regularization in Matrix Sensing: A Geometric View Leads to Stronger Results</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.07953">Nonasymptotic Guarantees for Low-Rank Matrix Recovery with Generative Priors</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.10981">The Global Geometry of Centralized and Distributed Low-rank Matrix Recovery without Regularization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.12795">The Landscape of Matrix Factorization Revisited</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01871">Iterative algorithm with structured diagonal Hessian approximation for solving nonlinear least squares problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01849">Rank $2r$ iterative least squares: efficient recovery of ill-conditioned low rank matrices from few entries</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.05484">Bridging Convex and Nonconvex Optimization in Robust PCA: Noise, Outliers, and Missing Data</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1911.13268">Adversarially Robust Low Dimensional Representations</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.05859">Fast and Robust Spectrally Sparse Signal Recovery: A Provable Non-Convex Approach via Robust Low-Rank Hankel Matrix Reconstruction</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.10020">Low-rank matrix recovery with composite optimization: good conditioning and rapid convergence</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.03275">Robust Subspace Recovery with Adversarial Outliers</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.07698">Noisy Matrix Completion: Understanding Statistical Guarantees for Convex Relaxation via Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.06116">Nonconvex Rectangular Matrix Completion via Gradient Descent without $\ell_{2, \infty}$ Regularization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.01631">Sharp Restricted Isometry Bounds for the Inexistence of Spurious Local Minima in Nonconvex Matrix Recovery</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1812.11466">Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Robust Principal Component Analysis</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.00404">An equivalence between stationary points for rank constraints versus low-rank factorizations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.11749">Iterative Hard Thresholding for Low-Rank Recovery from Rank-One Projections</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.09237">Nonconvex Robust Low-rank Matrix Recovery</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.00534">Run Procrustes, Run! On the convergence of accelerated Procrustes Flow</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.00904">Solving Systems of Quadratic Equations via Exponential-type Gradient Descent Algorithm</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.10251">How Much Restricted Isometry is Needed In Nonconvex Matrix Recovery?</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.07554">The Leave-one-out Approach for Matrix Completion: Primal and Dual Analysis</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06286">Nonconvex Matrix Factorization from Rank-One Measurements</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.09203">Algorithmic Regularization in Over-parameterized Matrix Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.05519">Accelerated Alternating Projections for Robust Principal Component Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02524">Provable quantum state tomography via non-convex methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01742">Memory-efficient Kernel PCA via Partial Matrix Sampling and Nonconvex Optimization: a Model-free Analysis of Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.08114">Nonconvex Low-Rank Matrix Recovery with Arbitrary Outliers via Median-Truncated Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.00257">Robust Principal Component Analysis by Manifold Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.09726">Spectral Compressed Sensing via Projected Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.08934">Reexamining Low Rank Matrix Factorization for Trace Norm Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03896">A Well-Tempered Landscape for Non-convex Robust Subspace Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.08683">Optimal Sample Complexity for Matrix Completion and Related Problems via $\ell_2$-Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.01265">Geometry of Factored Nuclear Norm Regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.00708">No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.09848">Painless Breakups – Efficient Demixing of Low Rank Matrices</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08651">Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimizations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.07945">Global Optimality in Low-rank Matrix Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06525">A Nonconvex Free Lunch for Low-Rank plus Sparse Matrix Recovery</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1612.09296">Symmetry, Saddle Points, and Global Geometry of Nonconvex Matrix Factorization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.03240">Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.07315">Nearly-optimal Robust Matrix Completion</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.01316">Provable non-convex projected gradient descent for a class of constrained matrix optimization problems</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.03168">Finding Low-rank Solutions to Matrix Problems, Efficiently and Provably</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.08370">Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07784">Fast Algorithms for Robust PCA via Gradient Descent</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07051">Convergence Analysis for Rectangular Matrix Completion Using Burer-Monteiro Factorization and Gradient Descent</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.07272">Matrix Completion has No Spurious Local Minimum</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.07221">Global Optimality of Local Search for Low Rank Matrix Recovery</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.06610">Guarantees of Riemannian Optimization for Low Rank Matrix Completion</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.02164">A Note on Alternating Minimization Algorithm for the Matrix Completion Problem</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.02262">Recovery guarantee of weighted low-rank approximation via alternating minimization</a> (2016)</li>
  <li><a href="http://dx.doi.org/10.1007/978-3-319-24486-0_1">Efficient Matrix Sensing Using Rank-1 Gaussian Measurements</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1511.01562">Guarantees of Riemannian Optimization for Low Rank Matrix Recovery</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1509.03025">Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1507.03566">Low-rank Solutions of Linear Matrix Equations via Procrustes Flow</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1506.06081">A Convergent Gradient Descent Algorithm for Rank Minimization and Semidefinite Programming from Random Linear Measurements</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1411.8003">Guaranteed Matrix Completion via Non-convex Factorization</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1411.1087">Fast Exact Matrix Completion with Finite Samples</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1410.7660">Non-convex Robust PCA</a> (2014)</li>
  <li><a href="http://jmlr.org/proceedings/papers/v35/hardt14a.pdf">Fast Matrix Completion without the Condition Number</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1312.0925">Understanding Alternating Minimization for Matrix Completion</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1212.0467">Low-rank Matrix Completion using Alternating Minimization</a> (2012)</li>
  <li><a href="http://arxiv.org/abs/0901.3150">Matrix Completion from a Few Entries</a> (2009)</li>
  <li><a href="http://arxiv.org/abs/0909.5457">Guaranteed Rank Minimization via Singular Value Projection</a> (2009)</li>
</ul>

<h3 id="tensor-recoverydecomposition--hidden-variable-models">Tensor Recovery/Decomposition &amp; Hidden Variable Models</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2104.14526">Scaling and Scalability: Provable Nonconvex Low-Rank Tensor Estimation from Incomplete Measurements</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.06234">Symmetry Breaking in Symmetric Tensor Decomposition</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2006.03134">Tensor Completion Made Practical</a> (2020)</li>
  <li><a href="https://doi.org/10.1007/s10107-020-01531-z">Optimization landscape of Tucker decomposition</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1911.09815">When Does Non-Orthogonal Tensor Decomposition Have No Spurious Local Minima?</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04436">Nonconvex Low-Rank Symmetric Tensor Completion from Noisy Data</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1811.12361">Smoothed Analysis in Unsupervised Learning via Decoupling</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.00148">Recovery Guarantees for Quadratic Tensors with Limited Observations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.00921">Algorithmic thresholds for tensor PCA</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.10348">Guaranteed Simultaneous Asymmetric Tensor Decomposition via Orthogonalized Alternating Least Squares</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.08204">A theory on the absence of spurious optimality</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.09326">Sparse and Low-rank Tensor Estimation via Cubic Sketchings</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1711.05424">The landscape of the spiked tensor model</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.04934">Statistically Optimal and Computationally Efficient Low Rank Tensor Completion from Noisy Entries</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.05598">On the Optimization Landscape of Tensor Decompositions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.02724">Tensor SVD: Statistical and Computational Limits</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.01804">Orthogonalized ALS: A Theoretically Principled Tensor Decomposition Algorithm for Practical Use</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06980">On Polynomial Time Methods for Exact Low Rank Tensor Completion</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1610.09322">Homotopy Method for Tensor PCA</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.01690">Low-tubal-rank Tensor Completion using Alternating Minimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1512.02337">Speeding up sum-of-squares for tensor decomposition and planted sparse vectors</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1510.04747">Tensor vs Matrix Methods: Robust Tensor Decomposition under Block Sparse Perturbations</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1411.1488">Analyzing Tensor Power Method Dynamics: Applications to Learning Overcomplete Latent Variable Models</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1210.7559">Tensor decompositions for learning latent variable models</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.0553">Provable Learning of Overcomplete Latent Variable Models: Semi-supervised and Unsupervised Settings</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1406.2784">Provable Tensor Factorization with Missing Data</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1402.5180">Guaranteed Non-Orthogonal Tensor Decomposition via Alternating Rank-1 Updates</a> (2014)</li>
</ul>

<h3 id="phase-retrieval">Phase Retrieval</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2103.04902">Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2011.00288">Optimal Sample Complexity of Gradient Descent for Amplitude Flow via Non-Lipschitz Matrix Concentration</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.01065">Hadamard Wirtinger Flow for Sparse Phase Retrieval</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.03238">Phase retrieval of complex-valued objects via a randomized Kaczmarz method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01066">On the Sample Complexity and Optimization Landscape for Quadratic Feasibility Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.02855">A Deterministic Convergence Framework for Exact Non-Convex Phase Retrieval</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1911.11301">The recovery of complex sparse signals from few phaseless measurements</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.12837">Online Stochastic Gradient Descent with Arbitrary Initialization Solves Non-smooth, Non-convex Phase Retrieval</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.09320">Solving Random Systems of Quadratic Equations with Tanh Wirtinger Flow</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.10358">On the Global Minimizers of Real Robust Phase Retrieval with Sparse Noise</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.10307">Solving a perturbed amplitude-based model for phase retrieval</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.02505">Spectral Method for Phase Retrieval: an Expectation Propagation Perspective</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.02676">Rigorous Analysis of Spectral Methods for Random Orthogonal Matrices</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.05612">Solving Complex Quadratic Systems with Full-Rank Random Matrices</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.03940">A Generalization of Wirtinger Flow for Exact Interferometric Inversion</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1808.05194">A Proximal Operator for Multispectral Phase Retrieval Problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.01255">Phase Retrieval by Alternating Minimization with Random Initialization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.04420">Optimal Spectral Initialization for Signal Recovery with Applications to Phase Retrieval</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.10520">Towards the optimal construction of a loss function without spurious local minima for solving quadratic equations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.02773">Solving systems of phaseless equations via Riemannian optimization with optimal sampling complexity</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.03547">Linear Spectral Estimators and an Application to Phase Retrieval</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.03276">Approximate Message Passing for Amplitude Based Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.07726">Gradient Descent with Random Initialization: Fast Global Convergence for Nonconvex Phase Retrieval</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.01170">Optimization-based AMP for Phase Retrieval: The Impact of Initialization and $\ell_2$-regularization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.03278">Compressive Phase Retrieval of Structured Signal</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.06245">Misspecified Nonconvex Statistical Optimization for Phase Retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.02426">Compressive Phase Retrieval via Reweighted Amplitude Flow</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.02083">A Local Analysis of Block Coordinate Descent for Gaussian Phase Retrieval</a> ([<span style="color:red"><strong>S</strong></span>], 2017)</li>
  <li><a href="https://arxiv.org/abs/1712.00716">Convolutional Phase Retrieval via Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.01712">Linear Convergence of An Iterative Phase Retrieval Algorithm with Data Reuse</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03247">The nonsmooth landscape of phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05234">Phase Retrieval via Linear Programming: Fundamental Limits and Algorithmic Improvements</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.10291">Convergence of the randomized Kaczmarz method for phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.09993">Phase Retrieval via Randomized Kaczmarz: Theoretical Guarantees</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.08167">Phase retrieval using alternating minimization in a batch setting</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.10407">Solving Almost all Systems of Random Quadratic Equations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.06412">Phase Retrieval Using Structured Sparsity: A Sample Efficient Algorithmic Framework</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02356">Solving (most) of a set of quadratic equalities: Composite optimization for robust phase retrieval</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.06256">Robust Wirtinger Flow for Phase Retrieval with Arbitrary Corruption</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.03286">Phase Retrieval via Sparse Wirtinger Flow</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06175">Structured signal recovery from quadratic measurements: Breaking sample complexity barriers via nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.07641">Sparse Phase Retrieval via Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.09540">Solving Large-scale Systems of Random Quadratic Equations via Stochastic Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1608.04141">Low Rank Phase Retrieval</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.03088">Phase retrieval with random Gaussian sensing vectors by alternating projections</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1607.08218">Non-Convex Phase Retrieval from STFT Measurements</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.08135">Gauss-Newton Method for Phase Retrieval</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.03196">Phase Retrieval via Incremental Truncated Wirtinger Flow</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.08285">Solving Systems of Random Quadratic Equations via Truncated Amplitude Flow</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.07719">Reshaped Wirtinger Flow for Solving Quadratic Systems of Equations</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.03805">Provable Non-convex Phase Retrieval with Outliers: Median Truncated Wirtinger Flow</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.06664">A Geometric Analysis of Phase Retrieval</a> ([<span style="color:red"><strong>S</strong></span>], 2016)</li>
  <li><a href="http://dx.doi.org/10.1109/TIT.2017.2672727">Phase retrieval for wavelet transforms </a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1506.07868">The Local Convexity of Solving Quadratic Equations</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1502.01822">Solving systems of phaseless equations via Kaczmarz methods: A proof of concept study</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1505.05114">Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1407.1065">Phase Retrieval via Wirtinger Flow: Theory and Algorithms</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1306.0160">Phase Retrieval using Alternating Minimization</a> (2013)</li>
</ul>

<h3 id="dictionary-learning">Dictionary Learning</h3>
<p>(See also <strong>Theory</strong> part in <a href="/research/dict-learn/">Dictionary/Deep Learning</a>)</p>

<ul>
  <li><a href="https://arxiv.org/abs/2002.10043">Complete Dictionary Learning via $\ell_p$-norm Maximization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1912.02427">Analysis of the Optimization Landscapes for Overcomplete Representation Learning</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.11167">Manifold Gradient Descent Solves Multi-Channel Sparse Blind Deconvolution Provably and Efficiently</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.10776">A Nonconvex Approach for Exact and Efficient Multichannel Sparse Blind Deconvolution</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1810.10702">Subgradient Descent Learns Orthogonal Dictionaries</a> ([<span style="color:red"><strong>S</strong></span>], 2018)</li>
  <li><a href="https://arxiv.org/abs/1809.10313">Efficient Dictionary Learning with Gradient Descent</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.08603">Towards Learning Sparsely Used Dictionaries with Arbitrary Supports</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1711.03638">A Provable Approach for Double-Sparse Coding</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03634">Alternating minimization for dictionary learning with random initialization</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1504.06785">Complete Dictionary Recovery over the Sphere</a> ([<span style="color:red"><strong>S</strong></span>], 2015)</li>
  <li><a href="http://arxiv.org/abs/1503.00778">Simple, Efficient, and Neural Algorithms for Sparse Coding</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1401.0579">More Algorithms for Provable Dictionary Learning</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1309.1952">Exact Recovery of Sparsely Used Overcomplete Dictionaries</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1308.6273">New Algorithms for Learning Incoherent and Overcomplete Dictionaries</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1310.7991">Learning Sparsely Used Overcomplete Dictionaries via Alternating Minimization</a> (2013)</li>
</ul>

<h3 id="deep-learning">Deep Learning</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2107.13289">Global minimizers, strict and non-strict saddle points, and implicit regularization for deep linear neural networks</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2008.11245">Deep Networks and the Multiple Manifold Problem</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.01429">The Global Landscape of Neural Networks: An Overview</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.01452">Modeling from Features: a Mean-field Framework for Over-parameterized Deep Neural Networks</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.05508">A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.01094">On the Global Convergence of Training Deep Linear ResNets</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.09526">Stochastic Subspace Cubic Newton Method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.09852">Training Linear Neural Networks: Non-Local Convergence and Complexity Results</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1912.08957">Optimization for deep learning: theory and algorithms</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04351">Stronger Convergence Results for Deep Residual Networks: Network Width Scales Linearly with Training Data Size</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.01413">Sub-Optimal Local Minima Exist for Almost All Over-parameterized Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.05874">Effects of Depth, Width, and Initialization: A Convergence Analysis of Layer-wise Training for Deep Linear Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.05505">Learning deep linear neural networks: Riemannian gradient flows and convergence to global minimizers</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.02419">Gradient Descent Finds Global Minima for Generalizable Deep Neural Networks of Practical Sizes</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.00193">A mean-field limit for certain deep neural networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.11955">On Exact Computation with an Infinitely Wide Neural Net</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.05263">Analysis of the Gradient Descent Algorithm for a Deep Neural Network Model with Skip-connections</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.07120">Training Over-parameterized Deep ResNet Is almost as Easy as Training a Two-layer Network</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.04440">Mean Field Analysis of Deep Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.00908">Stochastic Gradient Descent for Nonconvex Learning without Bounded Gradient Assumptions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.01384">A Generalization Theory of Gradient Descent for Learning Over-parameterized Deep ReLU Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.01028">Can SGD Learn Recurrent Neural Networks with Provable Generalization?</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08572">Width Provably Matters in Optimization for Deep Linear Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.00279">Elimination of All Bad Local Minima in Deep Learning</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1812.11039">Over-Parameterized Deep Neural Networks Have No Strict Local Minima For Any Continuous Activations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.08888">Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.08150">Effect of Depth and Width on Local Minima in Deep Learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.03962">A Convergence Theory for Deep Learning via Over-Parameterization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.03804">Gradient Descent Finds Global Minima of Deep Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.09038">Depth with Nonlinearity Creates No Bad Local Minima in ResNets</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02281">A Convergence Analysis of Gradient Descent for Deep Linear Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02032">Gradient descent aligns the layers of deep linear networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.08671">Adding One Neuron Can Eliminate All Bad Local Minima</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.06523">End-to-end Learning of a Convolutional Neural Network via Deep Tensor Decomposition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.00909">Understanding the Loss Surface of Neural Networks for Binary Classification</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06093">Gradient descent with identity initialization efficiently learns positive definite linear transformations by deep residual networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.01473">Deep linear neural networks with arbitrary loss: All local minima are global</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.02444">Global optimality conditions for deep neural networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08580">Depth Creates No Bad Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.00458">Electron-Proton Dynamics in Deep Learning</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1605.07110">Deep Learning without Poor Local Minima</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1605.08361">No bad local minima: Data independent training error guarantees for multilayer neural networks</a> (2016)</li>
</ul>

<h3 id="sparse-vectors-in-linear-subspaces">Sparse Vectors in Linear Subspaces</h3>
<p>(See <a href="/research/struct-elem/">Structured Element Pursuit</a> )</p>

<h3 id="nonnegativesparse-principal-component-analysis">Nonnegative/Sparse Principal Component Analysis</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1611.03819">Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1406.4775">Non-negative Principal Component Analysis: Message Passing Algorithms and Sharp Asymptotics</a> (2014)</li>
</ul>

<h3 id="mixed-linear-regression">Mixed Linear Regression</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1810.11874">Iteratively Learning from the Best</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.05752">Global Convergence of EM Algorithm for Mixtures of Two Component Linear Regression</a> (2018)</li>
  <li><a href="http://arxiv.org/abs/1608.05749">Solving a Mixture of Many Random Linear Equations by Tensor Decomposition and Alternating Minimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1412.3046">Provable Tensor Methods for Learning Mixtures of Classifiers</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1310.3745">Alternating Minimization for Mixed Linear Regression</a> (2013)</li>
</ul>

<h3 id="blind-deconvolutioncalibration">Blind Deconvolution/Calibration</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1905.12576">Global Guarantees for Blind Demodulation with Generative Priors</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.01913">On the Global Geometry of Sphere-Constrained Sparse Blind Deconvolution</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.01624">Composite optimization for robust blind deconvolution</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.00256">Geometry and Symmetry in Short-and-Sparse Deconvolution</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1809.06796">Nonconvex Demixing From Bilinear Measurements</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.00338">Structured Local Optima in Sparse Blind Deconvolution</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.10437">Global Geometry of Multichannel Sparse Blind Deconvolution on the Sphere</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.00111">Blind Gain and Phase Calibration via Sparse Spectral Methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.03309">Blind Deconvolution by a Steepest Descent Algorithm on a Quotient Manifold</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.08642">Regularized Gradient Descent: A Nonconvex Recipe for Fast Joint Blind Deconvolution and Demixing</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.04196">Self-Calibration via Linear Least Squares</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.09028">Through the Haze: A Non-Convex Approach to Blind Calibration for Linear Random Sensing Models</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1610.06469">Fast and guaranteed blind multichannel deconvolution under a bilinear system model</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1606.04933">Rapid, Robust, and Reliable Blind Deconvolution via Nonconvex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.02615">A Non-Convex Blind Calibration Method for Randomised Sensing Strategies</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1511.06146">RIP-like Properties in Subsampled Blind Deconvolution</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1511.06149">Blind Recovery of Sparse Signals from Subsampled Convolution</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1312.0525">Near Optimal Compressed Sensing of Sparse Rank-One Matrices via Sparse Power Factorization</a> (2013)</li>
</ul>

<h3 id="super-resolution">Super Resolution</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1811.12000">The basins of attraction of the global minimizers of the non-convex sparse spikes estimation problem</a> (2018)</li>
  <li><a href="http://arxiv.org/abs/1511.03385">Greed is Super: A Fast Algorithm for Super-Resolution</a> (2015)</li>
</ul>

<h3 id="synchronization-problemscommunity-detection">Synchronization Problems/Community Detection</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2106.15493">Generalized Power Method for Generalized Orthogonal Procrustes Problem: Global Convergence and Optimization Landscape Analysis</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2009.07514">A Unified Approach to Synchronization Problems over Subgroups of the Orthogonal Group</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.13638">Message Passing Least Squares Framework and its Application to Rotation Synchronization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.00902">Solving Orthogonal Group Synchronization via Convex and Low-Rank Optimization: Tightness and Landscape Analysis</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.05299">A Provably Robust Multiple Rotation Averaging Scheme for SO(2)</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1901.08235">Multi-Frequency Phase Synchronization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1809.11083">On the Landscape of Synchronization Networks: A Perspective from Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1703.06857">Near-optimal bounds for phase synchronization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1610.04583">Message-passing algorithms for synchronization problems over compact groups</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04426">On the low-rank approach for semidefinite programs arising in synchronization and community detection</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1601.06114">Nonconvex phase synchronization</a> (2016)</li>
</ul>

<h3 id="joint-alignment">Joint Alignment</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1609.05820">The Projected Power Method: An Efficient Algorithm for Joint Alignment from Pairwise Differences</a> (2016)</li>
</ul>

<h3 id="numerical-linear-algebra">Numerical Linear Algebra</h3>

<ul>
  <li><a href="https://arxiv.org/abs/1907.13602">Binary component decomposition Part II: The asymmetric case</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.13603">Binary Component Decomposition Part I: The Positive-Semidefinite Case</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1806.05151">On Landscape of Lagrangian Functions and Stochastic Search for Constrained Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.07459">PCA by Optimisation of Symmetric Functions has no Spurious Local Optima</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.04049">PCA by Determinant Optimization has no Spurious Local Optima</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1605.01036">Orbital minimization method with $\ell_1$ regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.01256">The Global Optimization Geometry of Nonsymmetric Matrix Factorization and Sensing</a> (2017)</li>
  <li><a href="http://arxiv.org/abs/1507.08366">On the matrix square root via geometric optimization</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1507.05854">Computing Matrix Squareroot via Non Convex Local Search</a> (2015)</li>
</ul>

<h3 id="bayesian-inference">Bayesian Inference</h3>
<ul>
  <li><a href="http://arxiv.org/abs/1503.06567">On some provably correct cases of variational inference for topic models</a> (2015)</li>
</ul>

<h3 id="empirical-risk-minimization--shallow-networks">Empirical Risk Minimization &amp; Shallow Networks</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2105.01778">Thinking Inside the Ball: Near-Optimal Minimization of the Maximal Loss</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2105.02375">A Geometric Analysis of Neural Collapse with Unconstrained Features</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2008.01805">Analytic Characterization of the Hessian in Shallow ReLU Models: A Tale of Symmetry</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.06731">Regularized linear autoencoders recover the principal components, eventually</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.13409">When Do Neural Networks Outperform Kernel Methods?</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.07928">Global Convergence of Sobolev Training for Overparametrized Neural Networks</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.06878">Optimization Theory for ReLU Neural Networks Trained with Normalization Layers</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.03991">Compressive sensing with un-trained neural networks: Gradient descent finds the smoothest approximation</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.11515">Nonconvex penalization for sparse neural networks</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.04486">Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.04026">Mean-Field Analysis of Two-Layer Neural Networks: Non-Asymptotic Rates and Generalization Bounds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.02882">Ill-Posedness and Optimization Geometry for Nonlinear Neural Network Training</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.02417">Near-Optimal Algorithms for Minimax Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.02208">Global Convergence of Frank Wolfe on One Hidden Layer Networks</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01987">A mean-field theory of lazy training in two-layer neural nets: entropic regularization and controlled McKean-Vlasov dynamics</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.00098">No Spurious Local Minima in Deep Quadratic Networks</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1912.02143">Landscape Complexity for the Empirical Risk of Generalized Linear Models</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.01599">Stationary Points of Shallow Neural Networks with Quadratic Activation Function</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.12360">How Much Over-parameterization Is Sufficient to Learn Deep ReLU Networks?</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.05059">Tight Sample Complexity of Learning One-hidden-layer Convolutional Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.05402">Quadratic number of nodes is sufficient to learn a dataset via gradient descent</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.14634">Denoising and Regularization via Exploiting the Structural Bias of Convolutional Generators</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.01663">Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.02333">Minimum “Norm” Neural Networks are Splines</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.01619">Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.12292">Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.09747">Finding the forward-Douglas-Rachford-forward method</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.03172">Towards Understanding the Importance of Noise in Training Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.05660">Effect of Activation Functions on the Training of Overparametrized Neural Nets</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.05368">Robust One-Bit Recovery via ReLU Generative Networks: Improved Statistical Rates and Global Landscape Analysis</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.05355">The generalization error of random features regression: Precise asymptotics and double descent curve</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.05520">The Landscape of Non-convex Empirical Risk with Degenerate Population Risk</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.08899">Limitations of Lazy Training of Two-layers Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.04326">A Comparative Analysis of the Optimization and Generalization Property of Two-layer Neural Network and Random Feature Models Under Gradient Descent Dynamics</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.11680">Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.07111">Global Convergence of Adaptive Gradient Methods for An Over-parameterized Neural Network</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.06015">Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.04674">Towards moderate overparameterization: global convergence guarantees for training shallow neural networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08584">Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.06827">A Deterministic Approach to Avoid Saddle Points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.06587">Fitting ReLUs via SGD and Quantized SGD</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.01375">Analysis of a Two-Layer Neural Network via Displacement Convexity</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1812.10004">Overparameterized Nonlinear Learning: Gradient Descent Takes the Shortest Path?</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.04176">A Provably Convergent Scheme for Compressive Sensing under Random Generative Priors</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.04918">Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.01885">Learning Two Layer Rectified Neural Networks in Polynomial Time</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.11059">Uniform Convergence of Gradients for Non-Convex Learning and Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.12065">On the Convergence Rate of Training Recurrent Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.06793"> Learning Two-layer Neural Networks with Symmetric Inputs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.03587">Algorithmic Aspects of Inverse Problems Using Generative Models</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.03592">ReLU Regression: Complexity, Exact and Approximation Algorithms</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02054">Gradient Descent Provably Optimizes Over-parameterized Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.03019">Stochastic Gradient Descent Learns State Equations with Nonlinear Activations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.04685">Learning ReLU Networks on Linearly Separable Data: Algorithm, Optimality, and Generalization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.01204">Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.07863">Learning ReLU Networks via Alternating Minimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.07808">Learning One-hidden-layer ReLU Networks via Gradient Descent</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.08855">Deep Denoising: Rate-Optimal Recovery of Structured Signals with a Deep Prior</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.07798">Improved Learning of One-hidden-layer Convolutional Neural Networks with Overlaps</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.04938">The Global Optimization Geometry of Shallow Linear Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.02677">Polynomial Convergence of Gradient Descent for Training One-Hidden-Layer Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.06561">A Mean Field View of the Landscape of Two-Layers Neural Networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.05012">Representing smooth functions as compositions of near-identity functions with implications for deep network optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.09319">SUNLayer: Stable denoising with generative networks</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.04304">Representation Learning and Recovery in the ReLU Model</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.02968">Learning Deep Models: Critical Points and Local Openness</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.01206">On the Power of Over-parametrization in Neural Networks with Quadratic Activation</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07301">On the Connection Between Learning Two-Layers Neural Networks and Tensor Decomposition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06463">Local Geometry of One-Hidden-Layer Neural Networks for Logistic Regression</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.03487">A Critical View of Global Optimality in Deep Learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.10132">The Multilinear Structure of ReLU Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.08968">Spurious Local Minima are Common in Two-Layer ReLU Neural Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.00779">Gradient Descent Learns One-hidden-layer CNN: Don’t be Afraid of Spurious Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.03440">Learning Non-overlapping Convolutional Neural Networks with Multiple Kernels</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.00501">Learning One-hidden-layer Neural Networks with Landscape Design</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11241">Theoretical properties of the global optimizer of two layer neural network</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11205">Critical Points of Neural Networks: Analytical Forms and Landscape Properties</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10174">SGD Learns Over-parameterized Networks that Provably Generalize on Linearly Separable Data</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.02196">Porcupine Neural Networks: (Almost) All Local Optima are Global</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.06129">When is a Convolutional Filter Easy To Learn?</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.04926">Theoretical insights into the optimization landscape of over-parameterized shallow neural networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03175">Recovery Guarantees for One-hidden-layer Neural Networks</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.09886">Convergence Analysis of Two-layer Neural Networks with ReLU Activation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.07576">Global Guarantees for Enforcing Deep Generative Priors by Empirical Risk</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.04591">Learning ReLUs via Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.07966">Globally Optimal Gradient Descent for a ConvNet with Gaussian Inputs</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1609.05191">Gradient Descent Learns Linear Dynamical Systems</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1607.06534">The Landscape of Empirical Risk for Non-convex Losses</a> (2016)</li>
</ul>

<h3 id="system-identification">System Identification</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1609.05191">Gradient Descent Learns Linear Dynamical Systems</a> (2016)</li>
</ul>

<h3 id="burer-monteiro-style-decomposition-algorithms">Burer-Monteiro Style Decomposition Algorithms</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1812.03046">Rank optimality for the Burer-Monteiro factorization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.03763">Smoothed analysis of the low-rank approach for smooth semidefinite programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.02008">Deterministic guarantees for Burer-Monteiro factorizations of smooth semidefinite programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.00186">Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1703.08729">Solving SDPs for synchronization and MaxCut problems via the Grothendieck inequality</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1611.03060">The Nonconvex Geometry of Low-Rank Matrix Optimizations with General Objective Functions</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.04970">The non-convex Burer-Monteiro approach works on smooth semidefinite programs</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04426">On the low-rank approach for semidefinite programs arising in synchronization and community detection</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1411.1134">Global Convergence of Stochastic Gradient Descent for Some Nonconvex Matrix Problems</a> (2014)</li>
</ul>

<h3 id="generic-structured-problems">Generic Structured Problems</h3>
<ul>
  <li><a href="https://arxiv.org/abs/1711.08172">Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1711.02621">Convex Optimization with Nonconvex Oracles</a> (2017)</li>
</ul>

<h3 id="nonconvex-feasibility-problems">Nonconvex Feasibility Problems</h3>

<ul>
  <li><a href="https://arxiv.org/abs/2011.08933">The alternating direction method of multipliers for finding the distance between ellipsoids</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1904.09148">The Douglas-Rachford Algorithm for Convex and Nonconvex Feasibility Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.08478">Finding magic squares with the Douglas-Rachford algorithm</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1709.05984">A convergent relaxation of the Douglas-Rachford algorithm</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.04846">A Lyapunov-type approach to convergence of the Douglas-Rachford algorithm</a> (2017)</li>
  <li><a href="https://carma.newcastle.edu.au/DRmethods/">Feasibility Problems: Douglas-Rachford and Projection Methods</a> (Project Page)</li>
  <li><a href="https://arxiv.org/abs/1610.03975">Dynamics of the Douglas-Rachford Method for Ellipses and p-Spheres</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1609.08751">A remark on the convergence of the Douglas-Rachford iteration in a non-convex setting</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1604.04657">On the finite convergence of the Douglas-Rachford algorithm for solving (not necessarily convex) feasibility problems in Euclidean spaces</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1506.09026">Global Behavior of the Douglas-Rachford Method for a Nonconvex Feasibility Problem</a> (2015)</li>
  <li><a href="http://dx.doi.org/10.1007/978-1-4419-9569-8_6">The Douglas–Rachford Algorithm in the Absence of Convexity</a> (2011)</li>
</ul>

<h2 id="of-statistical-nature-">Of Statistical Nature …</h2>

<ul>
  <li><a href="http://arxiv.org/abs/1609.04522">Sparse Tensorraphical Model: Non-convex Optimization and Statistical Inference</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1512.08269">Statistical and Computational Guarantees for the Baum-Welch Algorithm</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1502.01425">Provable Sparse Tensor Decomposition</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1501.00312">Statistical consistency and asymptotic normality for high-dimensional robust M-estimators</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1412.5632">Support recovery without incoherence: A case for nonconvex regularizations</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1412.8729">High Dimensional Expectation-Maximization Algorithm: Statistical Optimization and Asymptotic Normality</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.2156">Statistical guarantees for the EM algorithm: From population to sample-based analysis</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1408.5352">Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in Polynomial Time</a> (2014)</li>
  <li><a href="http://arxiv.org/abs/1305.2436">Regularized M-estimators with nonconvexity: Statistical and algorithmic theory for local ptima</a> (2013)</li>
  <li><a href="http://arxiv.org/abs/1109.3714">High-dimensional regression with noisy and missing data: Provable guarantees with nonconvexity</a> (2011)</li>
</ul>

<h2 id="relevant-optimization-methods-theory-miscs">Relevant Optimization Methods, Theory, Miscs</h2>

<ul>
  <li><a href="https://arxiv.org/abs/2112.01798">Convergence Properties of Monotone and Nonmonotone Proximal Gradient Methods Revisited</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2112.02572">Riemannian conjugate gradient methods: General framework and specific algorithms with convergence analyses</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2112.02089">Regularized Newton Method with Global $O(1/k^2)$ Convergence</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2112.02952">Gradient Regularization of Newton Method with Bregman Distances</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2111.14069">Escape saddle points by a simple gradient-descent based algorithm</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2111.13263">Negative curvature obstructs acceleration for geodesically convex optimization, even with exact first-order oracles</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2110.04814">Finding Second-Order Stationary Point for Nonconvex-Strongly-Concave Minimax Problem</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2110.03950">Nonconvex-Nonconcave Min-Max Optimization with a Small Maximization Domain</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2109.02455">Stochastic Subgradient Descent on a Generic Definable Function Converges to a Minimizer</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2108.10249">New Q-Newton’s method meets Backtracking line search: good convergence guarantee, saddle points avoidance, quadratic rate of convergence, and easy implementation</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2108.11832">Subgradient methods near active manifolds: saddle point avoidance, local convergence, and asymptotic normality</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2108.07963">Trust-region and $p$-regularized subproblems: local nonglobal minimum is the second smallest objective function value among all first-order stationary points</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2107.11231">Optimization on manifolds: A symplectic approach</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2107.11774">SGD May Never Escape Saddle Points</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2106.15044">A novel augmented Lagrangian method of multipliers for optimization with general inequality constraints</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2106.13340">Subgradient Ellipsoid Method for Nonsmooth Convex Problems</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2104.06206">An accelerated minimax algorithm for convex-concave saddle point problems with nonsmooth coupling function</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2104.06763">Oracle Complexity in Nonsmooth Nonconvex Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2104.04199">A Riemannian smoothing steepest descent method for non-Lipschitz optimization on submanifolds</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2104.02564">Hölder Gradient Descent and Adaptive Regularization Methods in Banach Spaces for First-Order Points</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2104.02519">The Impact of Noise on Evaluation Complexity: The Deterministic Trust-Region Case</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.15270">A Unifying Framework of Accelerated First-Order Approach to Strongly Monotone Variational Inequalities</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.15888">The Complexity of Nonconvex-Strongly-Concave Minimax Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.15989">Complexity of Projected Newton Methods for Bound-constrained Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.15993">A Proximal Quasi-Newton Trust-Region Method for Nonsmooth Regularized Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.03570">Second-order step-size tuning of SGD for non-convex optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.04568">Three Operator Splitting with a Nonconvex Loss Function</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2103.05138">On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.07346">On the Theory of Implicit Deep Learning: Global Convergence with Implicit Layers</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.09468">Don’t Fix What ain’t Broke: Near-optimal Local Convergence of Alternating Gradient Descent-Ascent for Minimax Optimization</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2102.01002">On Well-Structured Convex-Concave Saddle Point Problems and Variational Inequalities with Monotone Operators</a> (2021)</li>
  <li><a href="https://arxiv.org/abs/2012.11978">Finding Global Minima via Kernel Approximations</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2012.12936">Newton Acceleration on Manifolds identified by Proximal-Gradient Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2012.01474">Second-Order Guarantees in Federated Learning</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.13510">Spectral Residual Method for Nonlinear Equations on Riemannian Manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.13699">A Grassmann Manifold Handbook: Basic Geometry and Computational Aspects</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.09680">On the convergence of an improved discrete simulated annealing via landscape modification</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.08166">A Globally Convergent Proximal Newton-Type Method in Nonsmooth Convex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.03639">Graph cuts always find a global optimum (with a catch)</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.05474">Complexity of branch-and-bound and cutting planes in mixed-integer optimization – II</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.05023">Complexity of cutting planes and branch-and-bound in mixed-integer optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2011.00364">Efficient Methods for Structured Nonconvex-Nonconcave Min-Max Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.08908">Accelerated Algorithms for Convex and Non-Convex Optimization on Manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.10628">Limiting Behaviors of Nonconvex-Nonconcave Minimax Optimization via Continuous-Time Systems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.11379">Local Convergence Analysis of Augmented Lagrangian Methods for Piecewise Linear-Quadratic Composite Optimization Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.11737">Efficient Projection-Free Algorithms for Saddle Point Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.06097">Gradient Descent Ascent for Min-Max Problems on Riemannian Manifold</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.01618">Provable Acceleration of Neural Net Training via Polyak’s Momentum</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.02280">Solving strongly convex-concave composite saddle point problems with a small dimension of one of the variables</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.02653">QPALM: A Proximal Augmented Lagrangian Method for Nonconvex Quadratic Programs</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.03194">First-Order Algorithms Without Lipschitz Gradient: A Sequential Local Optimization Approach</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2010.03785">Riemannian Stochastic Variance-Reduced Cubic Regularized Newton Method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.13016">Escaping Saddle-Points Faster under Interpolation-like Conditions</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.09623">The Complexity of Constrained Min-Max Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.10551">A Generalized Newton Method for Subgradient Systems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.07153">Sequential Quadratic Optimization for Nonlinear Optimization Problems on Riemannian Manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2009.04014">Convergence Rate of A General Multi-Block ADMM in Nonconvex Nonsmooth Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.09919">Cubic Regularized Newton Method for Saddle Point Models: a Global and Local Convergence Analysis</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.11091">Unconstrained optimisation on Riemannian manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.07080">Iteration-complexity of a proximal augmented Lagrangian method for solving nonconvex composite optimization problems with nonlinear convex constraints</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.06557">On the globalization of Riemannian Newton method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.06314">A Note on the Finite Convergence of Alternating Projections</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.06148">Complexity aspects of local minima and related notions</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.02252">An accelerated first-order method for non-convex optimization on manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.01296">Faster Stochastic Alternating Direction Method of Multipliers for Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2008.02517">Curvature-Dependant Global Convergence Rates for Optimization on Manifolds of Bounded Geometry</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.12219">A First-Order Primal-Dual Method for Nonconvex Constrained Optimization Based On the Augmented Lagrangian</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.11426">A proximal gradient method for control problems with nonsmooth and nonconvex control cost</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.10525">Sequential Quadratic Optimization for Nonlinear Equality Constrained Stochastic Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.04528">Higher-order methods for convex-concave min-max optimization and monotone variational inequalities</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.14901">Understanding Notions of Stationarity in Non-Smooth Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.15226">Riemannian Optimization on the Symplectic Stiefel Manifold</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.00242">Random projection of Linear and Semidefinite problem with linear inequalities</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2007.01284">Rate-improved Inexact Augmented Lagrangian Method for Constrained Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.11336">Behavior of Limited Memory BFGS when Applied to Nonsmooth Functions and their Nesterov Smoothings</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.13476">Second-Order Information in Non-Convex Stochastic Optimization: Power and Limitations</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.14592">Newton-type Methods for Minimax Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.11144">On the Almost Sure Convergence of Stochastic Gradient Descent in Non-Convex Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.12363">A Second-order Equilibrium in Nonconvex-Nonconcave Min-max Optimization: Existence and Algorithm</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.09065">The limits of min-max optimization algorithms: convergence to spurious non-critical sets</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.07458">Projection Robust Wasserstein Distance and Riemannian Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.08183">Nonconvex Optimization Tools for Large-Scale Matrix and Tensor Decomposition with Structured Factors</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.07925">A Line-Search Descent Algorithm for Strict Saddle Functions with Complexity Guarantees</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.08141">Non-convex Min-Max Optimization: Applications, Challenges, and Recent Theoretical Advances</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.08548">A Note on Nesterov’s Accelerated Method in Nonconvex Optimization: a Weak Estimate Sequence Approach</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.08667">The Landscape of Nonconvex-Nonconcave Minimax Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.09033">Two steps at a time – taking GAN training in stride with Tseng’s method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.09263">Accelerated Primal-Dual Algorithms for a Class of Convex-Concave Saddle-Point Problems with Non-Bilinear Coupling Term</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.09734">Asymptotic stationarity and regularity for nonsmooth optimization problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.07278">Convergence for nonconvex ADMM, with applications to CT imaging</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.06946">SGD with shuffling: optimal rates without component convexity and large epoch requirements</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.06889">Fast Objective and Duality Gap Convergence for Non-convex Strongly-concave Min-max Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.06650">Convergence of adaptive algorithms for weakly convex constrained optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.02559">Nonmonotone Globalization for Anderson Acceleration Using Adaptive Regularization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.00719">ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.01512">A modification of quasi-Newton’s methods helping to avoid saddle points</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.00423">A New Accelerated Stochastic Gradient Method with Momentum</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.02032">A Unified Single-loop Alternating Gradient Projection Algorithm for Nonconvex-Concave and Convex-Nonconcave Minimax Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2006.02067">Generalization Bounds for Stochastic Saddle Point Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.06844">An SQP method for equality constrained optimization on manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.05238">FedSplit: An algorithmic framework for fast federated optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.06976">Multilevel Riemannian optimization for low-rank problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.02356">Manifold Proximal Point Algorithms for Dual Principal Component Pursuit and Orthogonal Dictionary Learning</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.01209">Riemannian Stochastic Proximal Gradient Methods for Nonsmooth Optimization over the Stiefel Manifold</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.00224">Distributed Stochastic Non-Convex Optimization: Momentum-Based Variance Reduction</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2005.01378">High-Dimensional Robust Mean Estimation via Gradient Descent</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.14866">New Results on Superlinear Convergence of Classical Quasi-Newton Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.12301">Blind Data Detection in Massive MIMO via $\ell_3$-norm Maximization over the Stiefel Manifold</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.08657">On Tight Convergence Rates of Without-replacement SGD</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.06840">On Dissipative Symplectic Integration with Applications to Gradient-Based Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.06479">Faster Stochastic Quasi-Newton Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.02210">Regularized asymptotic descents for nonconvex optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.13807">Explicit Regularization of Stochastic Gradient Methods through Duality</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.14366">Second-Order Guarantees in Centralized, Federated and Decentralized Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.13607">Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2004.00041">Likelihood landscape and maximum likelihood estimation for the discrete orbit recovery model</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.11758">Breaking the $O(1/\varepsilon)$ Optimal Rate for a Class of Minimax Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.11238">Zeroth-order Optimization on Riemannian Manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.10576">Symmetry &amp; critical points for a model neural network</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.09174">Rates of Superlinear Convergence for Classical Quasi-Newton Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.09960">Efficient Clustering for Stretched Mixtures: Landscape and Optimality</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.08880">Augmented Lagrangian based first-order methods for convex and nonconvex programs: nonergodic convergence and iteration complexity</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.08093">Solving Non-Convex Non-Differentiable Min-Max Games using Proximal Gradient Method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.07612">Variable Smoothing for Weakly Convex Composite Functions</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.03963">A block inertial Bregman proximal algorithm for nonsmooth nonconvex problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.04546">First-Order Methods for Nonconvex Quadratic Minimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.04375">A Primal Dual Smoothing Framework for Max-Structured Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.03910">Geometry of First-Order Methods and Adaptive Acceleration</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.03502">A quadratically convergent proximal algorithm for nonnegative tensor decomposition</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.03709">Generative Adversarial Imitation Learning with Neural Networks: Global Optimality and Convergence Rate</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.02395">On the Convergence of Adam and Adagrad</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2003.01469">A Riemannian Newton Optimization Framework for the Symmetric Tensor Rank Approximation Problem</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11962">Can We Find Near-Approximately-Stationary Points of Nonsmooth Nonconvex Functions?</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11323">Convergence to Second-Order Stationarity for Non-negative Matrix Factorization: Provably and Concurrently</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11384">Intrinsic Construction of Lyapunov Functions on Riemannian Manifold</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11337">Fast Linear Convergence of Randomized BFGS</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11582">Proximal Gradient Algorithm with Momentum and Flexible Parameter Restart for Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.11875">Optimality and Stability in Non-Convex-Non-Concave Min-Max Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.12266">SPRING: A fast stochastic proximal alternating method for non-smooth non-convex optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.09018">Second Order Optimization Made Practical</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.08513">A Trust-Region Method For Nonsmooth Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.07919">Efficient Search of First-Order Nash Equilibria in Nonconvex-Concave Smooth Min-Max Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.07290">Stochastic Gauss-Newton Algorithms for Nonconvex Compositional Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.06848">SingCubic: Cyclic Incremental Newton-type Gradient Descent with Cubic Regularization for Non-Convex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.06277">A mean-field analysis of two-player zero-sum games</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.06694">Structures of Spurious Local Minima in k-means</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.05359">Adaptivity of Stochastic Gradient Methods for Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.05309">Sharp Analysis of Epoch Stochastic Gradient Descent Ascent Methods for Min-Max Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.05466">Convergence of a Stochastic Gradient Method with Momentum for Nonsmooth Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.04144">Practical Accelerated Optimization on Riemannian Manifolds</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.04130">On Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.03687">SPAN: A Stochastic Projected Approximate Newton Method</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.03329">Better Theory for SGD in the Nonconvex World</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.00657">Greedy Quasi-Newton Methods with Explicit Superlinear Convergence</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01644">Hybrid Riemannian Conjugate Gradient Methods with Global Convergence Properties</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.01004">On the Optimal Combination of Tensor Optimization Methods</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2002.00057">Last Iterate is Slower than Averaged Iterate in Smooth Convex-Concave Saddle Point Problems</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.11994">Consensus-Based Optimization on the Sphere I: Well-Posedness and Mean-Field Limit</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.11988">Consensus-based Optimization on the Sphere II: Convergence to Global Minimizers and Machine Learning</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.10802">Strong Evaluation Complexity Bounds for Arbitrary-Order Optimization of Nonconvex Nonsmooth Composite Functions</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.10669">A Stochastic Subgradient Method for Nonsmooth Nonconvex Multi-Level Composition Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.10114">Second-order Online Nonconvex Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.08876">From Nesterov’s Estimate Sequence to Riemannian Acceleration</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/2001.00216">Introduction to Nonsmooth Analysis and Optimization</a> (2020)</li>
  <li><a href="https://arxiv.org/abs/1912.09564">The Douglas–Rachford Algorithm Converges Only Weakly</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.11347">Robust Group Synchronization via Cycle-Edge Message Passing</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.06508">A Distributed Quasi-Newton Algorithm for Primal and Dual Regularized Empirical Risk Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.07079">Semismooth Newton-type method for bilevel optimization: Global convergence and extensive numerical experiments</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.07146">Active strict saddles in nonsmooth optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.07481">On Lower Iteration Complexity Bounds for the Saddle Point Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.07527">Leveraging Two Reference Functions in Block Bregman Proximal Gradient Descent for Non-convex and Non-Lipschitz Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.07580">Convergence of a Stochastic Subgradient Method with Averaging for Nonsmooth Nonconvex Constrained Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.04456">A Stochastic Quasi-Newton Method for Large-Scale Nonconvex Optimization with Applications</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.04365">Trust-Region Newton-CG with Strong Second-Order Complexity Guarantees for Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.04617">Second-Order Non-Convex Optimization for Constrained Fixed-Structure Static Output Feedback Controller Synthesis</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.02365">Lower Bounds for Non-Convex Stochastic Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.12518">Analysis of Asymptotic Escape of Strict Saddle Sets in Manifold Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.02767">Efficient Semidefinite Programming with approximate ADMM</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.02516">Local convergence of tensor methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.02093">Implementing a smooth exact penalty function for general constrained nonlinear optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.02039">Stochastic proximal splitting algorithm for stochastic composite minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.01745">Polynomial time guarantees for the Burer-Monteiro method</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1912.00137">Proximal Splitting Algorithms: Overrelax them all!</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.12545">An accelerated first-order method with complexity analysis for solving cubic regularization subproblems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.09900">An inexact augmented Lagrangian method for nonsmooth optimization on Riemannian manifold</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.10367">A Stochastic Tensor Method for Non-convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.11955">Hölderian error bounds and Kurdyka-Łojasiewicz inequality for the trust region subproblem</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.06920">A Fully Stochastic Second-Order Trust Region Method</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.07596">Convergence Analysis of a Momentum Algorithm with Adaptive Step Size for Non Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.08526">The nonsmooth landscape of blind deconvolution</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.09195">On the tightness of SDP relaxations of QCQPs with repeated eigenvalues</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.08510">Optimal Complexity and Certification of Bregman First-Order Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.05047">Nonsmooth Optimization over Stiefel Manifold: Riemannian Subgradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04443">Bundle Method Sketching for Low Rank Semidefinite Programming</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04076">Second-order optimality conditions for non-convex set-constrained optimization problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.05167">Nonconvex Stochastic Nested Optimization via Stochastic ADMM</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.06317">Gradientless Descent: High-Dimensional Zeroth-Order Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04334">Stochastic Difference-of-Convex Algorithms for Solving nonconvex optimization problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.04584">Regularization of Limited Memory Quasi-Newton Methods for Large-Scale Nonconvex Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1911.06284">Primal-dual block-proximal splitting for a class of non-convex problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.13857">UniXGrad: A Universal, Adaptive Algorithm with Optimal Guarantees for Constrained Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.13742">Unifying mirror descent and dual averaging</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.13604">Pathological subgradient dynamics</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.13852">Linear Speedup in Saddle-Point Escape for Decentralized Non-Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.12375">Towards a theory of non-commutative optimization: geodesic first and second order methods for moment maps and polytopes</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.12166">Improved Zeroth-Order Variance Reduced Algorithms and Analysis for Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.10879">Convergence Rates of Subgradient Methods for Quasi-convex Optimization Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.09488">Relative Interior Rule in Block-Coordinate Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.09373">A Stochastic Extra-Step Quasi-Newton Method for Nonsmooth Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.08590">Anderson Acceleration of Proximal Gradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.08156">Zero Duality Gap in View of Abstract Convexity</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.11199">A nonsmooth nonconvex descent algorithm</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.05544">A generalized Douglas-Rachford splitting algorithm for nonconvex optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.06000">On the Convergence of Perturbed Distributed Asynchronous Stochastic Gradient Descent to Second Order Stationary Points in Non-convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.03296">A Global Newton-Type Scheme Based on a Simplified Newton-Type Approach</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.04384">Circumcentering Reflection Methods for Nonconvex Feasibility Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.04366">Understanding Limitation of Two Symmetrized Orders by Worst-case Complexity</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.04300">Implementing a smooth exact penalty function for equality-constrained nonlinear optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.04194">Nonconvex stochastic optimization on manifolds via Riemannian Frank-Wolfe methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.02280">Convergence Analysis of Gradient Algorithms on Riemannian Manifolds Without Curvature Constraints and Application to Riemannian Mass</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.01845">The Complexity of Finding Stationary Points with Stochastic Gradient Descent</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1910.00115">First-order primal-dual methods for nonsmooth non-convex optimisation</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.10455">Necessary and Sufficient Conditions for Adaptive, Mirror, and Standard Gradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.06946">A Stochastic Proximal Point Algorithm for Saddle-Point Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.06065">Riemannian Proximal Gradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.05485">Extending FISTA to Riemannian Optimization for Sparse PCA</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.04799">The chain rule for VU-decompositions of nonsmooth functions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.04248">An Average Curvature Accelerated Composite Gradient Method for Nonconvex Smooth Composite Optimization Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.00171">Near-optimal Approximate Discrete and Continuous Submodular Function Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.11518">Inexact Proximal-Point Penalty Methods for Non-Convex Optimization with Non-Convex Constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.11482">Anderson Accelerated Douglas-Rachford Splitting</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.00470">Accelerating ADMM for Efficient Simulation and Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1909.00918">Efficiency of Coordinate Descent Methods For Structured Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.09941">Stochastic Optimization for Non-convex Inf-Projection Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.10150">Sparse solutions of optimal control via Newton method for under-determined systems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.10525">Linear Convergence of Adaptive Stochastic Gradient Descent</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.10935">Randomly initialized EM algorithm for two-component Gaussian mixture achieves near optimality in $O(\sqrt{n})$ iterations</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.07023">Second-Order Guarantees of Stochastic Gradient Descent in Non-Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.05699">Convergence Behaviour of Some Gradient-Based Methods on Bilinear Games</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.05406">On the behaviour of the Douglas-Rachford algorithm for minimizing a convex function subject to a linear constraint</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.00745">On The Geometric Analysis of A Quartic-quadratic Optimization Problem under A Spherical Constraint</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.00865">Gradient Flows and Accelerated Proximal Splitting Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.01089">Path Length Bounds for Gradient Descent and Flow</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.02022">Fenchel Duality for Convex Optimization and a Primal Dual Algorithm on Riemannian Manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.01871">Proximally Constrained Methods for Weakly Convex Optimization with Weakly Convex Constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.02734">Proximal Point Methods for Optimization with Nonconvex Functional Constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.02747">Distributed Gradient Descent: Nonconvergence to Saddle Points and the Stable-Manifold Theorem</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.11357">An Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.11687">Incremental Methods for Weakly Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1908.00131">Complexity of Proximal Augmented Lagrangian for nonconvex optimization with nonlinear equality constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.13023">On Inexact Solution of Auxiliary Problems in Tensor Methods for Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.11742">A simple Newton method for local nonsmooth optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.08226">Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked Matrix-Tensor Model</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.08843">The Generalized Trust Region Subproblem: solution complexity and convex hull results</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.10531">Sampling and Optimization on Convex Sets in Riemannian Manifolds of Non-Negative Curvature</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.09547">Stochastic algorithms with geometric step decay converge linearly on sharp functions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.09697">Heavy-ball Algorithms Always Escape Saddle Points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.06140">Bilevel Optimization and Variational Analysis</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.04450">SNAP: Finding Approximate Second-Order Stationary Solutions Efficiently for Non-convex Linearly Constrained Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.05388">Provably Efficient Reinforcement Learning with Linear Function Approximation</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.01848">Distributed Learning in Non-Convex Environments – Part I: Agreement at a Linear Rate</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.01849">Distributed Learning in Non-Convex Environments – Part II: Polynomial Escape from Saddle-Points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.01543">Efficient Algorithms for Smooth Minimax Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.00949">Optimization on flag manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.11985">Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.00113">Learning Markov models via low-rank optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1907.01145">The generalized orthogonal Procrustes problem in the high noise regime</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.06776">Global Convergence of Least Squares EM for Demixing Two Log-Concave Densities</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.08383">Global Convergence of Policy Gradient Methods to (Almost) Locally Optimal Policies</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.10436">Riemannian optimization on the simplex of positive definite matrices</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.10053">Block-coordinate and incremental aggregated nonconvex proximal gradient methods: a unified view</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.07772">First-order methods almost always avoid saddle points: the case of vanishing step-sizes</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.07355">Escaping from saddle points on Riemannian manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.05975">Iteration-complexity and asymptotic analysis of steepest descent method for multiobjective optimization on Riemannian manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.04321">Efficiently escaping saddle points on manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.03622">Accelerated Alternating Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.01115">Proximal Point Approximations Achieving a Convergence Rate of $O(1/k)$ for Smooth Convex-Concave Saddle Point Problems: Optimistic Gradient and Extra-gradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1906.00331">On Gradient Descent Ascent for Nonconvex-Concave Minimax Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.07821">The algorithm by Ferson et al. is surprisingly fast: An NP-hard optimization problem solvable in almost linear time with high probability</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.11904">Bregman forward-backward splitting for nonconvex composite optimization: superlinear convergence to nonisolated critical points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.09992">Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.10027">Neural Temporal-Difference Learning Converges to Global Optima</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.10018">Momentum-Based Variance Reduction in Non-Convex SGD</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.07010">A FISTA-type accelerated gradient algorithm for solving smooth nonconvex composite optimization problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1905.00529">Stabilized SVRG: Simple Variance Reduction for Nonconvex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.09265">SSRGD: Simple Stochastic Recursive Gradient Descent for Escaping Saddle Points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.09712">Provable Bregman-divergence based Methods for Nonconvex and Non-Lipschitz Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.10112">Stochastic Primal-Dual Algorithms with Faster Convergence than $O(1/\sqrt{T})$ for Problems without Bilinear Structure</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.11295">Bregman Proximal Gradient Algorithm with Extrapolation for a class of Nonconvex Nonsmooth Minimization Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.07147">Burer-Monteiro guarantees for general semidefinite programs</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.06784">A Trust Region Method for Finding Second-Order Stationarity in Linearly Constrained Non-Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1904.01517">Convergence rates for the stochastic gradient descent method for non-convex objective functions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.11576">An Alternating Manifold Proximal Gradient Method for Sparse PCA and Sparse CCA</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.08634">Aggressive Local Search for Constrained Optimal Control Problems with Many Local Minima</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.08110">Online Non-Convex Learning: Following the Perturbed Leader is Optimal</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.08619">The importance of better models in stochastic optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.03471">Limited-Memory BFGS with Displacement Aggregation</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.00184">Proximal algorithms for constrained composite optimization, with applications to solving low-rank SDPs</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.01463">SGD without Replacement: Sharper Rates for General Smooth Convex Functions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.01540">A Stochastic Trust Region Method for Non-convex Minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.01932">Escaping Saddle Points with the Successive Convex Approximation Algorithm</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1903.01818">Inertial Block Mirror Descent Method for Non-Convex Non-Smooth Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.10406">Minimization of nonsmooth nonconvex functions using inexact evaluations and its worst-case complexity</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.10767">High-Order Evaluation Complexity for Convexly-Constrained Optimization with Non-Lipschitzian Group Sparsity Terms</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.07815">Analysis of the alternating direction method of multipliers for nonconvex problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.07672">Stochastic Proximal Gradient Methods for Non-smooth Non-Convex Regularized Problems</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.04811">Stochastic Gradient Descent Escapes Saddle Points Efficiently</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.01144">Adaptive stochastic gradient algorithms on Riemannian manifolds</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.01710">Inexact restoration with subsampled trust-region methods for finite-sum minimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.01903">Exponentiated Gradient Meets Gradient Descent</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.02060">A Convergence Analysis of Nonlinearly Constrained ADMM in Deep Learning</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.02715">Momentum Schemes with Stochastic Variance Reduction for Nonconvex Composite Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.00247">Sharp Analysis for Nonconvex SGD Escaping from Saddle Points</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1902.00139">Passed \&amp; Spurious: analysing descent algorithms and local minima in spiked matrix-tensor model</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.11518">Stochastic Recursive Variance-Reduced Cubic Regularization Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.11224">Lower Bounds for Smooth Nonconvex Finite-Sum Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08958">Perturbed Proximal Descent to Escape Saddle Points for Non-convex and Non-smooth Objective Functions</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.09149">Escaping Saddle Points with Adaptive Gradient Methods</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.10000">Simple algorithms for optimization on Riemannian manifolds with constraints</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.10682">On the Convergence of (Stochastic) Gradient Descent with Extrapolation for Non-Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.10791">Fast Gradient Methods for Symmetric Nonnegative Matrix Factorization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.10269">An accelerated variant of simulated annealing that converges under fast cooling</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08428">Cheap Orthogonal Constraints in Neural Networks: A Simple Parametrization of the Orthogonal and Unitary Group</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08511">A Unified Analysis of Extra-gradient and Optimistic Gradient Methods for Saddle Point Problems: Proximal Point Approach</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.08369">Stochastic Gradient Methods for Non-Smooth Non-Convex Regularized Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.07634">DTN: A Learning Rate Scheme with Convergence Rate of $O(1/t)$ for SGD</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.07487">Non-Asymptotic Analysis of Fractional Langevin Monte Carlo for Non-Convex Optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1901.02746">Primal-dual proximal splitting and generalized conjugation in non-smooth non-convex optimization</a> (2019)</li>
  <li><a href="https://arxiv.org/abs/1812.10229">A Proximal Alternating Direction Method of Multiplier for Linearly Constrained Nonconvex Minimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.07643">Semi-Riemannian Manifold Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.07725">Breaking Reversibility Accelerates Langevin Dynamics for Global Non-Convex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1812.02878">Solving Non-Convex Non-Concave Min-Max Games Under Polyak-Łojasiewicz Condition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.11378">A Doubly Accelerated Inexact Proximal Point Method for Nonconvex Composite Optimization Problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.11590">Convergence Analysis of the Relaxed Douglas-Rachford Algorithm</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.11637">Adaptive Stochastic Variance Reduction for Subsampled Newton Method with Cubic Regularization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.11829">Stochastic Optimization for DC Functions and Non-smooth Non-convex Regularizers with Non-asymptotic Convergence</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.08990">Markov Chain Block Coordinate Descent</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.08109">Faster First-Order Methods for Stochastic Non-Convex Optimization on Riemannian Manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.07057">Universal regularization methods - varying the power, the smoothness and the accuracy</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.08413">Sampling Can Be Faster Than Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.04644">Blind Over-the-Air Computation and Data Fusion via Provable Wirtinger Flow</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.03831">Deterministic and stochastic inexact regularization algorithms for nonconvex optimization with optimal complexity</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.04194">R-SPIDER: A Fast Riemannian Stochastic Optimization Algorithm with Curvature Independent Rate</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.00980">Proximal Gradient Method for Manifold Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.01220">Sharp worst-case evaluation complexity bounds for arbitrary-order nonconvex optimization with inexpensive constraints</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.01298">Inexact alternating projections on nonconvex sets</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1811.02564">On exponential convergence of SGD in non-convex over-parametrized learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.13387">A Stochastic Penalty Model for Convex and Nonconvex Optimization with Big Constraints</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.08530">Provably Correct Automatic Subdifferentiation for Qualified Programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.12361">Global Non-convex Optimization with Discretized Diffusions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.11344">Benefits of over-parameterization with EM</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.11636">Newton method for finding a singularity of a special class of locally Lipschitz continuous vector fields on Riemannian manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.11640">Inexact Newton method with feasible inexact projections for solving constrained smooth and nonsmooth equations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.10207">Solving Weakly-Convex-Weakly-Concave Saddle-Point Problems as Weakly-Monotone Variational Inequality</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.10690">SpiderBoost: A Class of Faster Variance-reduced Algorithms for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.07590">Uniform Graphical Convergence of Subgradients in Nonconvex Optimization and Learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.07211">A Subsampling Line-Search Method with Second-Order Results</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02893">Optimization on Spheres: Models and Proximal Algorithms with Computational Performance Comparisons</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.03229">Analytical Convergence Regions of Accelerated First-Order Methods in Nonconvex Optimization under Regularity Condition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.03763">Cubic Regularization with Momentum for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.03930">Parallelizable Algorithms for Optimization Problems with Orthogonality Constraints</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.00553">Optimal Adaptive and Accelerated Stochastic Gradient Descent</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.00760">Riemannian Adaptive Optimization Methods</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.00303">Newton-MR: Newton’s Method Without Smoothness or Convexity</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02024">Convergence to Second-Order Stationarity for Constrained Non-Convex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1810.02060">Non-Convex Min-Max Optimization: Provable Algorithms and Applications in Machine Learning</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.09449">Hessian barrier algorithms for linearly constrained optimization problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.09853">Stochastic Second-order Methods for Non-convex Optimization with Inexact Hessian and Gradient</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.06704">An Inexact First-order Method for Constrained Nonlinear Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.07181">Survey: Sixty Years of Douglas–Rachford</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.04564">On the Stability and Convergence of Stochastic Gradient Descent with Momentum</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.06474">Zeroth-order (Non)-Convex Stochastic Optimization via Conditional Gradient and Gradient Updates</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.04198">Optimization with Non-Differentiable Constraints with Applications to Fairness, Recall, Churn, and Other Goals</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.02162">Escaping Saddle Points in Constrained Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.05895">Primal-dual accelerated gradient descent with line search for convex and nonconvex optimization problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.05527">Secondary gradient descent in higher codimension</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.04216">On Markov Chain Gradient Descent</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.04618">Global Convergence of Stochastic Gradient Hamiltonian Monte Carlo for Non-Convex Stochastic Optimization: Non-Asymptotic Performance Bounds and Momentum-Based Acceleration</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1809.00452">Structured Quasi-Newton Methods for Optimization with Orthogonality Constraints</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.05671">On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.06239">Theoretical study of an adaptive cubic regularization method with dynamic inexact Hessian information</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.06274">Iteration-Complexity of the Subgradient Method on Riemannian Manifolds with Lower Bounded Curvature</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.07382">Convergence of Cubic Regularization for Nonconvex Optimization under KL Property</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.07384">A Note on Inexact Condition for Cubic Regularized Newton’s Method</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.04839">Discrete gradient descent differs qualitatively from gradient flow</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.02901">Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1808.02941">On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1807.07563">A linear-time algorithm for generalized trust region problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1807.07554">A geometric integration approach to nonsmooth, nonconvex optimisation</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1807.04428">Convergence Rate of Block-Coordinate Maximization Burer-Monteiro Method for Solving Large SDPs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1807.01695">SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.06373">Geodesic Convex Optimization: Differentiation on Manifolds, Geodesics, and Convexity</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.04781">On the Convergence Rate of Stochastic Mirror Descent for Nonsmooth Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.01811">AdaGrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.00900">Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.02694">Gradient Method for Optimization on Riemannian Manifolds with Lower Bounded Curvature</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.02812">Towards Riemannian Accelerated Gradient Methods</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1806.00065">Adaptive regularization with cubics on manifolds with a first-order analysis</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.09357">Minimizing Nonconvex Population Risk from Rough Empirical Risk</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.08756">On the Connection Between Sequential Quadratic Programming and Riemannian Gradient Methods</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.08370">Cutting plane methods can be extended into nonconvex optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.07960">Stochastic Gradient Descent for Stochastic Doubly-Nonconvex Composite Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.09416">Adaptive Stochastic Gradient Langevin Dynamics: Taming Convergence and Saddle Point Escape Time</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.06444">A geometric integration approach to smooth optimisation: Foundations of the discrete gradient method</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.05565">A Cubic Regularized Newton’s Method over Riemannian Manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.05411">Accelerated Stochastic Algorithms for Nonconvex Finite-sum and Multi-block Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1805.05751">Local Saddle Point Optimization: A Curvature Exploitation Approach</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.11003">Gradient Sampling Methods for Nonsmooth Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.07795">Stochastic subgradient method converges on tame functions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.08739">An Envelope for Davis-Yin Splitting and Strict Saddle Point Avoidance</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.09629">Convergence guarantees for a class of non-convex and non-smooth optimization problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.02907">On the spherical quasi-convexity of quadratic functions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1804.01076">Operator Scaling via Geodesically Convex Optimization, Invariant Theory and Polynomial Identity Testing</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.08600">Lower error bounds for the stochastic gradient descent optimization algorithm: Sharp convergence rates for slowly and fast decaying learning rates</a> (2018)</li>
  <li><a href="https://doi.org/10.1137/17M1127582">A Riemannian BFGS Method Without Differentiated Retraction for Nonconvex Optimization Problems</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.03943">Nonconvex weak sharp minima on Riemannian manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1803.02924">A Newton-CG Algorithm with Complexity Guarantees for Smooth Unconstrained Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07796">Continuous Relaxation of MAP Inference: A Nonconvex Perspective</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.10418">On the Sublinear Convergence of Randomly Perturbed Alternating Gradient Descent to Second Order Stationary Solutions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.09592">ADMM for Multiaffine Constrained Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.09128">Averaging Stochastic Gradient Descent on Riemannian Manifolds</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.08941">Gradient Primal-Dual Algorithm Converges to Second-Order Stationary Solutions for Nonconvex Distributed Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07843">Concise Complexity Analyses for Trust-Region Methods</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02988">Stochastic subgradient method converges at the rate $O(k^{-1/4})$ on weakly convex functions</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06509">On the Optimization of Deep Networks: Implicit Acceleration by Overparameterization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.08246">Characterizing Implicit Bias in Terms of Optimization Geometry</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06439">Local Optimality and Generalization Guarantees for the Langevin Algorithm via Empirical Metastability</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06384">Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.07372">Sample Complexity of Stochastic Variance-Reduced Cubic Regularization for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.06175">An Alternative View: When Does SGD Escape Local Minima?</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.05155">Toward Deeper Understanding of Nonconvex Stochastic Optimization with Momentum using Diffusion Approximations</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.03889">Convergence Analysis of Alternating Nonconvex Projections</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02628">Manifold Optimization Over the Set of Doubly Stochastic Matrices: A Second-Order Geometry</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.02688">Exact Semidefinite Formulations for a Class of (Random and Non-Random) Nonconvex Quadratic Programs</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1802.01062">How to Characterize the Worst-Case Performance of Algorithms for Nonconvex Optimization</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.09387">On the Quadratic Convergence of the Cubic Regularization Method under a Local Error Bound Condition</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.01994">The proximal alternating direction method of multipliers in the nonconvex setting: convergence analysis and rates</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1801.03013">Nonconvex Lagrangian-Based Optimization: Monitoring Schemes and Global Convergence</a> (2018)</li>
  <li><a href="https://arxiv.org/abs/1712.06585">Third-order Smoothness Helps: Even Faster Stochastic Optimization Algorithms for Finding Local Minima</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.03950">Saving Gradient and Negative Curvature Computations: Finding Local Minima More Efficiently</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1712.01033">NEON+: Accelerated Gradient Methods for Extracting Negative Curvature for Non-Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.10456">Accelerated Gradient Descent Escapes Saddle Points Faster than Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.10467">Implicit Regularization in Nonconvex Statistical Estimation: Gradient Descent Converges Linearly for Phase Retrieval, Matrix Completion and Blind Deconvolution</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.06673">Neon2: Finding Local Minima via First-Order Oracles</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.08172">Run-and-Inspect Method for Nonconvex Optimization and Global Optimality Bounds for R-Local Minimizers</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.05224">Revisiting Normalized Gradient Descent: Evasion of Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02838">Stochastic Cubic Regularization for Fast Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.02621">Convex Optimization with Nonconvex Oracles</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01944">First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.01303">On local non-global minimizers of quadratic functions with cubic regularization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10770">Frank-Wolfe methods for geodesically convex optimization with application to the matrix geometric mean</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.10329">Lower Bounds for Higher-Order Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1711.00841">Lower Bounds for Finding Stationary Points II: First-Order Methods</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.11606">Lower Bounds for Finding Stationary Points I</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.09447">Stochastic Non-convex Optimization with Strong High Probability Second-order Convergence</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.09047">Block Coordinate Descent Only Converge to Minimizers</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.07406">First-order Methods Almost Always Avoid Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05338">Accelerated Block Coordinate Proximal Gradients with Applications in High Dimensional Statistics</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.05017">The power of sum-of-squares for detecting hidden structures</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1710.02236">Primal-Dual Optimization Algorithms over Riemannian Manifolds: an Iteration Complexity Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.08571">On Noisy Negative Curvature Descent: Competing with Gradient Descent for Faster Non-convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.07180">Worst-case evaluation complexity and optimality of second-order methods for nonconvex smooth optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.05747">Douglas-Rachford splitting and ADMM for nonconvex optimization: new convergence results and accelerated versions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.05758">Local Minimizers and Second-Order Conditions in Composite Piecewise Programming via Directional Derivatives</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1709.04451">Alternating minimization and alternating descent over nonconvex sets</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.07827">Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.07164">Newton-Type Methods for Non-Convex Optimization Under Inexact Hessian Information</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.04783">Non-convex Conditional Gradient Sliding</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.04044">Improved second-order evaluation complexity for unconstrained nonlinear optimization using high-order regularized models</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1708.00475">An Inexact Regularized Newton Framework with a Worst-Case Iteration Complexity of $O(\varepsilon^{-3/2})$ for Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1707.08028">A Second Order Method for Nonconvex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.07993">Behavior of Accelerated Gradient Methods Near Critical Points of Nonconvex Problems</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.05681">Mirror descent in non-convex stochastic programming</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03131">Complexity analysis of second-order line-search algorithms for smooth nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.04097">Provable Alternating Gradient Descent for Non-negative Matrix Factorization with Strong Correlations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.03267">An Alternative to EM for Gaussian Mixture Models: Batch and Stochastic Riemannian Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1706.00896">Using Negative Curvature in Solving Nonlinear Programs</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.10412">Gradient Descent Can Take Exponential Time to Escape Saddle Points</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.07285">Optimality of orders one to three and beyond: characterization and evaluation complexity in constrained nonconvex optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.05933">Sub-sampled Cubic Regularization for Non-convex Optimization</a> (2017)</li>
  <li><a href="https://doi.org/10.1007/s10957-017-1093-4">Iteration-Complexity of Gradient, Subgradient and Proximal Point Methods on Riemannian Manifolds</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02502">Linearized ADMM for Non-convex Non-smooth Optimization with Convergence Analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1705.02766">“Convex Until Proven Guilty”: Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.08227">Accelerating Stochastic Gradient Descent</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1704.04548">On the Gap Between Strict-Saddles and True Convexity: An $\Omega(\log d)$ Lower Bound for Eigenvector Approximation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.10993">Catalyst Acceleration for Gradient-Based Non-Convex Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.07915">Perspective: Energy Landscapes for Machine Learning</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.07755">Gradient descent with nonconvex constraints: local concavity determines convergence</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.04890">Riemannian stochastic quasi-Newton algorithm with variance reduction and its convergence analysis</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00887">How to Escape Saddle Points Efficiently</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00329">Convergence rate of a simulated annealing algorithm with noisy observations</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1703.00412">Exploiting Negative Curvature in Deterministic and Stochastic Optimization</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.08134">Online Multiview Representation Learning: Dropping Convexity for Better Efficiency</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.00763">Natasha: Faster Stochastic Non-Convex Optimization via Strongly Non-Convex Parameter</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1702.06435">Phase Transitions of Spectral Initialization for High-Dimensional Nonconvex Estimation</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1701.06501">Maximum likelihood estimation of determinantal point processes</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1701.04271">Fast Rates for Empirical Risk Minimization of Strict Saddle Problems</a> (2017)</li>
  <li><a href="https://arxiv.org/abs/1612.00547">Gradient Descent Efficiently Finds the Cubic-Regularized Non-Convex Newton Step</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.04831">The Power of Normalization: Faster Evasion of Saddle Points</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.00756">Accelerated Methods for Non-Convex Optimization</a> (2016)</li>
  <li><a href="https://dx.doi.org/10.1007/s10107-016-1065-8">Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1611.01146">Finding Local Minima for Nonconvex Optimization in Linear Time</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1609.07428">Convergence Rate Analysis of a Stochastic Trust Region Method for Nonconvex Optimization</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1608.07630">Global analysis of Expectation Maximization for mixtures of two Gaussians</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1609.00978">Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1606.09070">Local Convergence of the Heavy-ball Method and iPiano for Non-convex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.08101">Global rates of convergence for nonconvex optimization on manifolds</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1605.00405">Gradient Descent Converges to Minimizers: The Case of Non-Isolated Critical Points</a> (2016)</li>
  <li><a href="https://arxiv.org/abs/1604.04603">On the Douglas-Rachford algorithm</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1603.04064">A Grothendieck-type inequality for local maxima</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.06053">First-order Methods for Geodesically Convex Optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.05908">Efficient approaches for escaping higher order saddle points in non-convex optimization</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1602.04915">Gradient Descent Converges to Minimizers</a> (2016)</li>
  <li><a href="http://arxiv.org/abs/1510.06096">When Are Nonconvex Problems Not Scary?</a> ([<span style="color:red"><strong>S</strong></span>], 2015; see also <a href="/docs/thesis.pdf">my thesis</a>)</li>
  <li><a href="http://www.optimization-online.org/DB_HTML/2014/11/4635.html">On the Global Optimality for Linear Constrained Rank Minimization Problem</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1511.06324">Global Convergence of ADMM in Nonconvex Nonsmooth Optimization</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1509.03917">Dropping Convexity for Faster Semi-definite Optimization</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1508.04468">Local Linear Convergence of the ADMM/Douglas–Rachford Algorithms without Strong Convexity and Application to Statistical Imaging</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1501.07242">Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions</a> (2015)</li>
  <li><a href="http://arxiv.org/abs/1503.02101">Escaping From Saddle Points — Online Stochastic Gradient for Tensor Decomposition</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1507.00887">Peaceman-Rachford splitting for a class of nonconvex optimization problems</a> (2015)</li>
  <li><a href="https://arxiv.org/abs/1409.8444">Douglas-Rachford splitting for nonconvex optimization with application to nonconvex feasibility problems</a> (2014)</li>
  <li><a href="https://arxiv.org/abs/1407.0753">Global convergence of splitting methods for nonconvex composite optimization</a> (2014)</li>
  <li><a href="https://arxiv.org/abs/1310.3787">Accelerated Gradient Methods for Nonconvex Nonlinear and Stochastic Programming</a> (2013)</li>
  <li><a href="http://dx.doi.org/10.1007/s10107-013-0701-9">Proximal alternating linearized minimization for nonconvex and nonsmooth problems</a> (2013)</li>
  <li><a href="http://dx.doi.org/10.1007/s10107-011-0484-9">Convergence of descent methods for semi-algebraic and tame problems: proximal algorithms, forward–backward splitting, and regularized Gauss–Seidel methods</a> (2013)</li>
  <li><a href="http://www.optimization-online.org/DB_HTML/2012/07/3535.html">Optimality conditions for the nonlinear programming problems on Riemannian manifolds</a> (2012)</li>
  <li><a href="https://doi.org/10.1007/s10589-009-9240-y">Second-order negative-curvature methods for box-constrained and general constrained optimization</a> (2010)</li>
  <li><a href="http://arxiv.org/abs/0801.1780">Proximal alternating minimization and projection methods for nonconvex problems: An approach based on the Kurdyka-Łojasiewicz inequality</a> (2008)</li>
  <li><a href="https://dx.doi.org/10.1007/s10107-006-0706-8">Cubic regularization of Newton method and its global performance</a> (2006)</li>
  <li><a href="http://dx.doi.org/10.1137/030602290">Computing the Local-Nonglobal Minimizer of a Large Scale Trust-Region Subproblem</a> (2005)</li>
  <li><a href="http://dx.doi.org/10.1137/S1052623494278049">On Some Properties of Quadratic Programs with a Convex Quadratic Constraint</a> (1998)</li>
  <li><a href="http://dx.doi.org/10.1137/0804009">Local minimizers of quadratic functions on Euclidean balls and spheres</a> (1994)</li>
</ul>

<blockquote>
  <p><strong>Disclaimer</strong> - This page is meant to serve a hub for references on this problem, and does not represent in any way personal endorsement of papers listed here. So I do not hold any responsibility for quality and technical correctness of each paper listed here. The reader is advised to use this resource with discretion.</p>
</blockquote>

<blockquote>
  <p><strong>If you’d like your paper to be listed here</strong>  - Just drop me a few lines via email (which can be found on “Welcome” page). If you don’t bother to spend a word, just deposit your paper on arXiv. I get email alert about new animals there every morning,  and will be happy to hunt one for this zoo if it seems <strong>fit</strong>.</p>
</blockquote>

<blockquote>
  <p><strong>Special thanks to</strong>: <a href="https://people.orie.cornell.edu/dsd95/">Damek Davis</a>, <a href="http://www.math.ucla.edu/~wotaoyin/">Wotao Yin</a>, <a href="https://angel.co/vladislav-voroninski">Vladislav Voroninski</a>, David Martinezhttps://waset.org/child-and-adolescent-psychiatry-conference-in-december-2021-in-paris</p>
</blockquote>

  </article>

<!--  -->

<!--  -->

</div>

      </div>
    </div>

    <footer>

  <div class="wrapper">
    &copy; Copyright 2026 Ju Sun.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
  </div>

</footer>


    <!--load Ubuntu webfont -->

<link href='http://fonts.googleapis.com/css?family=Ubuntu|Ubuntu+Condensed|Ubuntu+Mono' rel='stylesheet' type='text/css'>

<!-- mathjax config-->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX", "output/HTML-CSS"],
  tex2jax: {
    inlineMath: [ ['$', '$'], ['\\(','\\)']],
    displayMath: [ ['$$', '$$'], ['\\[','\\]']],
    processEscapes: true,
    processEnvironments: true, 
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
    TeX: { equationNumbers: { autoNumber: "AMS", useLabelIds: true } }, 
  messageStyle: "none",
  "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
